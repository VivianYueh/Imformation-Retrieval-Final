{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivianYueh/Imformation-Retrieval-Final/blob/main/%E9%97%9C%E9%8D%B5%E5%AD%97%E6%93%B7%E5%8F%96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>此處是使用處理中文句子斷詞的範例，包括CKIPtagger及Distiltag</h3>"
      ],
      "metadata": {
        "id": "7vVfZfoxqu6-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AuqXp259rhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1e5a0b-72f2-4174-fbc5-706fc79ff33a"
      },
      "source": [
        "!pip install -U DistilTag"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting DistilTag\n",
            "  Using cached DistilTag-0.2.2-py3-none-any.whl.metadata (390 bytes)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (from DistilTag) (5.2.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from DistilTag) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=3.2 in /usr/local/lib/python3.11/dist-packages (from DistilTag) (4.51.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from DistilTag) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->DistilTag) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown->DistilTag) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown->DistilTag) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6->DistilTag) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.2->DistilTag) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.2->DistilTag) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.2->DistilTag) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.2->DistilTag) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->DistilTag) (1.7.1)\n",
            "Using cached DistilTag-0.2.2-py3-none-any.whl (15 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, DistilTag\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed DistilTag-0.2.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvpYnakT9ro0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56fa7ff3-d1d4-4ed2-9ba2-2042e96522e0"
      },
      "source": [
        "import DistilTag\n",
        "DistilTag.download()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1AzUICPQ5MMt_IWg4JZ3mWM6vGbQkv01L\n",
            "From (redirected): https://drive.google.com/uc?id=1AzUICPQ5MMt_IWg4JZ3mWM6vGbQkv01L&confirm=t&uuid=844a4526-217c-47f9-b9f9-dd92981e6851\n",
            "To: /tmp/tmpsam65fw6distiltag/tagmodel.zip\n",
            "100%|██████████| 501M/501M [00:10<00:00, 46.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting up model...\n",
            "DistilTag model installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUIHi3IW9rr0"
      },
      "source": [
        "from DistilTag import DistilTag\n",
        "tagger = DistilTag()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 保留名詞、動詞"
      ],
      "metadata": {
        "id": "Wjnx1i8xPI4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_nouns_verbs(tagged):\n",
        "    \"\"\"\n",
        "    過濾出名詞(N*)與動詞(V*)的詞性標註\n",
        "    \"\"\"\n",
        "    return [(word, pos) for word, pos in tagged if pos.startswith('N') or pos.startswith('V')]\n"
      ],
      "metadata": {
        "id": "L-qLfT1UOBel"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 擷取關鍵字"
      ],
      "metadata": {
        "id": "iHbE_QlCPpe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_words(words):\n",
        "    return list(dict.fromkeys(words))  # 去重同時保留原順序"
      ],
      "metadata": {
        "id": "JHF8Q8VrT7pr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "base_url = 'https://www.niar.org.tw'\n",
        "news_base_url = 'https://www.niar.org.tw/xmdoc?xsmsid=0I148622737263495777'\n",
        "\n",
        "page = 1\n",
        "news_count = 0\n",
        "\n",
        "print(\"國家實驗研究院新聞列表：\\n\")\n",
        "\n",
        "while news_count<10:\n",
        "    url = f\"{news_base_url}&page={page}\"\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    rows = soup.find_all('tr')\n",
        "\n",
        "    found_news = False\n",
        "\n",
        "    for row in rows:\n",
        "        date_td = row.find('td', class_='date')\n",
        "        title_td = row.find('td', class_='title')\n",
        "\n",
        "        if date_td and title_td:\n",
        "            found_news = True\n",
        "            news_count += 1\n",
        "            date = date_td.text.strip()\n",
        "            a_tag = title_td.find('a')\n",
        "            title = a_tag.text.strip()\n",
        "            tags = tagger.tag(title)  # 這裡可能是 list[list[tuple]]\n",
        "            filtered = [filter_nouns_verbs(sent) for sent in tags]\n",
        "            words = [word for sent in filtered for word, pos in sent]\n",
        "            unique = unique_words(words)\n",
        "            link = base_url + a_tag['href']\n",
        "            print(f\"{news_count}. [{date}] {title}\\n   {tags}\\n   {unique}\\n   👉 {link}\\n\")\n",
        "\n",
        "    if not found_news:\n",
        "        break  # 沒有新內容就停止\n",
        "    page += 1"
      ],
      "metadata": {
        "id": "2_y__vUpr9TI",
        "outputId": "a3aad1e3-f498-4b26-90fb-7cfb60ed38a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "國家實驗研究院新聞列表：\n",
            "\n",
            "1. [2025-04-16] 科教館「科學家的秘密基地」 4/16更新展區重新亮相\n",
            "   [[('科教館', 'Nc'), ('「', 'PARENTHESISCATEGORY'), ('科學家', 'Na'), ('的', 'DE'), ('秘密', 'Na'), ('基地', 'Nc'), ('」', 'PARENTHESISCATEGORY'), ('４／１６', 'Nd'), ('更新', 'VC'), ('展區', 'Nc'), ('重新', 'D'), ('亮相', 'VA')]]\n",
            "   ['科教館', '科學家', '秘密', '基地', '４／１６', '更新', '展區', '亮相']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P106529706322578269\n",
            "\n",
            "2. [2025-04-14] 【國研院說明稿】關於小油坑失火事件  若鑑定結果為國研院設備造成 國研院絕不卸責\n",
            "   [[('【', 'PARENTHESISCATEGORY'), ('國研院', 'Nc'), ('說明稿', 'Na'), ('】', 'PARENTHESISCATEGORY'), ('關於', 'P'), ('小油坑', 'Nc'), ('失火', 'VH'), ('事件', 'Na'), ('若', 'Cbb'), ('鑑定', 'VC'), ('結果', 'Na'), ('為', 'P'), ('國研院', 'Nc'), ('設備', 'Na'), ('造成', 'VG'), ('國研院', 'Nc'), ('絕不', 'D'), ('卸責', 'VA')]]\n",
            "   ['國研院', '說明稿', '小油坑', '失火', '事件', '鑑定', '結果', '設備', '造成', '卸責']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P104744411632087760\n",
            "\n",
            "3. [2025-04-14] 【國研院說明稿】關於小油坑失火事件\n",
            "   [[('【', 'PARENTHESISCATEGORY'), ('國研院', 'Nc'), ('說明稿', 'Na'), ('】', 'PARENTHESISCATEGORY'), ('關於', 'P'), ('小油坑', 'Nc'), ('失火', 'VH'), ('事件', 'Na')]]\n",
            "   ['國研院', '說明稿', '小油坑', '失火', '事件']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P104624748119528484\n",
            "\n",
            "4. [2025-04-11] 臺法電子資訊與器官晶片深化交流 合作提升雙方科技研發量能\n",
            "   [[('臺', 'Nc'), ('法', 'Nc'), ('電子', 'Na'), ('資訊', 'Na'), ('與', 'Caa'), ('器官', 'Na'), ('晶片', 'Na'), ('深化', 'VHC'), ('交流', 'VH'), ('合作', 'VH'), ('提升', 'VC'), ('雙方', 'Nh'), ('科技', 'Na'), ('研發', 'Nv'), ('量能', 'Na')]]\n",
            "   ['臺', '法', '電子', '資訊', '器官', '晶片', '深化', '交流', '合作', '提升', '雙方', '科技', '研發', '量能']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P104350250093579190\n",
            "\n",
            "5. [2025-04-01] 國立陽明交通大學、振興醫院與國研院國儀中心 共同開發新型微米級光纖麥克風 為聽障者提供更無礙的聲音感受\n",
            "   [[('國立', 'A'), ('陽明', 'Nb'), ('交通', 'Na'), ('大學', 'Nc'), ('、', 'PAUSECATEGORY'), ('振興', 'VC'), ('醫院', 'Nc'), ('與', 'Caa'), ('國研院', 'Nc'), ('國儀', 'Na'), ('中心', 'Nc'), ('共同', 'A'), ('開發', 'VC'), ('新型', 'Na'), ('微米級', 'Na'), ('光纖', 'Na'), ('麥克風', 'Na'), ('為', 'P'), ('聽障者', 'Na'), ('提供', 'VD'), ('更', 'D'), ('無礙', 'VH'), ('的', 'DE'), ('聲音', 'Na'), ('感受', 'Na')]]\n",
            "   ['陽明', '交通', '大學', '振興', '醫院', '國研院', '國儀', '中心', '開發', '新型', '微米級', '光纖', '麥克風', '聽障者', '提供', '無礙', '聲音', '感受']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P091383779861602246\n",
            "\n",
            "6. [2025-04-01] TAIWAN AI RAP試營運啟動 徵求試營運用戶，免費算力助攻AI開發！\n",
            "   [[('ＴＡＩＷＡＮＡＩＲＡＰ', 'FW'), ('試', 'VF'), ('營運', 'Nv'), ('啟動', 'Na'), ('徵求', 'VC'), ('試', 'Nv'), ('營運', 'Nv'), ('用戶', 'Na'), ('，', 'COMMACATEGORY')], [('免費', 'VH'), ('算力', 'Na'), ('助攻', 'VA'), ('ＡＩ', 'DASHCATEGORY'), ('開發', 'VC'), ('！', 'EXCLANATIONCATEGORY')]]\n",
            "   ['試', '營運', '啟動', '徵求', '用戶', '免費', '算力', '助攻', '開發']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P091372518985390226\n",
            "\n",
            "7. [2025-03-28] 114年度「公民團體創新示範與沙盒試驗計畫」 徵件說明會\n",
            "   [[('１１４年度', 'Nd'), ('「', 'PARENTHESISCATEGORY'), ('公民', 'Na'), ('團體', 'Na'), ('創新', 'VC'), ('示範', 'Nv'), ('與', 'Caa'), ('沙盒', 'Na'), ('試驗', 'Na'), ('計畫', 'Na'), ('」', 'PARENTHESISCATEGORY'), ('徵件', 'Na'), ('說明', 'Na'), ('會', 'D')]]\n",
            "   ['１１４年度', '公民', '團體', '創新', '示範', '沙盒', '試驗', '計畫', '徵件', '說明']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P087618875405814531\n",
            "\n",
            "8. [2025-03-25] 國家實驗研究院英文名稱更名揭牌\n",
            "   [[('國家', 'Na'), ('實驗', 'Na'), ('研究院', 'Nc'), ('英文', 'Na'), ('名稱', 'Na'), ('更名', 'VA'), ('揭牌', 'VA')]]\n",
            "   ['國家', '實驗', '研究院', '英文', '名稱', '更名', '揭牌']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P084523018471266747\n",
            "\n",
            "9. [2025-03-24] 國網中心晶創主機Nano 5徵案啟動 南台灣半導體業搶搭高效能運算列車\n",
            "   [[('國網', 'Na'), ('中心', 'Nc'), ('晶創', 'Nb'), ('主機', 'Na'), ('Ｎａｎｏ５', 'FW'), ('徵案', 'Na'), ('啟動', 'VC'), ('南', 'Ncd'), ('台灣', 'Nc'), ('半導體業', 'Na'), ('搶搭', 'VC'), ('高效能', 'Na'), ('運算', 'VC'), ('列車', 'Na')]]\n",
            "   ['國網', '中心', '晶創', '主機', '徵案', '啟動', '南', '台灣', '半導體業', '搶搭', '高效能', '運算', '列車']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P083555590774151742\n",
            "\n",
            "10. [2025-03-19] 國研院x國資圖 科學家的秘密基地@臺中 「地震工程大解密－讓建築更安全的秘密」地震科普展 房屋耐震，大家一起來！\n",
            "   [[('國研院', 'Nc'), ('ｘ', 'DASHCATEGORY'), ('國資圖', 'Na'), ('科學家', 'Na'), ('的', 'DE'), ('秘密', 'Na'), ('基地', 'Nc'), ('＠', 'FW'), ('臺中', 'Nc'), ('「', 'PARENTHESISCATEGORY'), ('地震', 'Nv'), ('工程', 'Na'), ('大解密', 'VA'), ('－', 'FW'), ('讓', 'VL'), ('建築', 'Na'), ('更', 'D'), ('安全', 'VH'), ('的', 'DE'), ('秘密', 'Na'), ('」', 'PARENTHESISCATEGORY'), ('地震科', 'Nc'), ('普展', 'VC'), ('房屋', 'Na'), ('耐震', 'VH'), ('，', 'COMMACATEGORY')], [('大家', 'Nh'), ('一起', 'D'), ('來', 'VA'), ('！', 'EXCLANATIONCATEGORY')]]\n",
            "   ['國研院', '國資圖', '科學家', '秘密', '基地', '臺中', '地震', '工程', '大解密', '讓', '建築', '安全', '地震科', '普展', '房屋', '耐震', '大家', '來']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P078409728113064068\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 內文擷取"
      ],
      "metadata": {
        "id": "5RQh2-aiU6DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import textwrap\n",
        "\n",
        "base_url = 'https://www.niar.org.tw'\n",
        "news_base_url = 'https://www.niar.org.tw/xmdoc?xsmsid=0I148622737263495777'\n",
        "\n",
        "page = 1\n",
        "news_count = 0\n",
        "\n",
        "print(\"國家實驗研究院新聞列表：\\n\")\n",
        "\n",
        "while news_count<10:\n",
        "    url = f\"{news_base_url}&page={page}\"\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    rows = soup.find_all('tr')\n",
        "\n",
        "    found_news = False\n",
        "\n",
        "    for row in rows:\n",
        "        date_td = row.find('td', class_='date')\n",
        "        title_td = row.find('td', class_='title')\n",
        "\n",
        "        if date_td and title_td:\n",
        "            found_news = True\n",
        "            news_count += 1\n",
        "            #date = date_td.text.strip()\n",
        "            a_tag = title_td.find('a')\n",
        "            title = a_tag.text.strip()\n",
        "            '''tags = tagger.tag(title)  # 這裡可能是 list[list[tuple]]\n",
        "            filtered = [filter_nouns_verbs(sent) for sent in tags]\n",
        "            words = [word for sent in filtered for word, pos in sent]\n",
        "            unique = unique_words(words)'''\n",
        "            link = base_url + a_tag['href']\n",
        "            article_response = requests.get(link)\n",
        "            article_response.encoding = 'utf-8'\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "            # 抓取文章內容\n",
        "            article_response = requests.get(link)\n",
        "            article_response.encoding = 'utf-8'\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "            content = article_soup.find_all('p', class_='MsoNormal')\n",
        "            content_text = \"\\n\".join(textwrap.fill(p.get_text(strip=True), width=80)  for p in content  if p.get_text(strip=True))\n",
        "            print(f\"{news_count}. {title}\\n   {content_text}\\n\")\n",
        "            #print(f\"{news_count}. [{date}] {title}\\n   {tags}\\n   {unique}\\n   👉 {link}\\n\")\n",
        "\n",
        "    if not found_news:\n",
        "        break  # 沒有新內容就停止\n",
        "    page += 1"
      ],
      "metadata": {
        "id": "2a8iguHYU9N8",
        "outputId": "2283b20c-92f8-47ec-b02c-eb3f6e317a76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "國家實驗研究院新聞列表：\n",
            "\n",
            "1. 科教館「科學家的秘密基地」 4/16更新展區重新亮相\n",
            "   國科會轄下之國家實驗研究院及國家太空中心，與國立臺灣科學教育館合作，辦理「科學家的秘密基地」長期展，自2023年3月開展以來，備受各界好評，已吸引超過13萬人入\n",
            "場參觀。今（4/16）日於更新部分展品後重新開展，希望能幫助參觀民眾透過互動遊戲、親眼觀察與模型展示，習得有趣的科學知識，同時深入認識科研工作。「科學家的秘密基\n",
            "地」位於科教館8樓東南側扇形展場，分為「實驗基地」、「探測基地」和「智慧基地」三區，每一區均包含兩大展示主題。此次更新的是國家生物模式中心（原國家實驗動物中心）\n",
            "、國家地震工程研究中心（國震中心）及台灣海洋科技研究中心（海洋中心）的展品。一進門就看到實驗基地－生物模式中心展出的「貓咪毛色變化的秘密」，這裡展示多基因協同作\n",
            "用的觀念。生物模式中心近年以革新的基因編輯工具「CRISPR/Cas」基因剪刀產製「擬人鼠」動物模式，推進我國精準醫療研究與臨床應用，而科學家進行基因編輯時，須\n",
            "嚴加考慮生物體複雜的基因層次。現場除了深入淺出的科學觀念，也設計了一個基因密碼轉盤遊戲，參觀者可以透過簡單的科普工具，輕鬆學習生物醫學知識。實驗基地－國震中心透\n",
            "過互動展示，介紹房屋的共振原理，包括不同樓高建築物的振動特性，以及地震波在不同地質條件下傳遞所產生的共振效應。現場提供以不同長度的吸管模擬各種樓高的共振教具，讓\n",
            "參觀者能親身體驗結構與地震波共振的現象。同時展示「滾動式隔震平台」，結合電動振動台模擬真實地震波，可清楚觀察放置於隔震裝置上的設備，其振動幅度大幅減少，讓參觀者\n",
            "直觀感受隔震技術的保護效果。接著來到探測基地－海洋中心，這裡展出於臺灣西南海域及南海採集到的海洋浮游動物標本、寫真、採集工具，以及使用海洋中心自製底碇平台記錄之\n",
            "水下300米的海底世界。海洋浮游動物在海洋生態系中扮演著重要而多樣的角色，牠們的種類及數量非常容易受到海洋環境的影響，當環境變動時（如溫度、鹽度、酸鹼度），浮游\n",
            "動物的組成也會隨之改變，間接對魚類、鯨類等各種海洋動物產生連鎖反應。因此浮游動物是一種很好的環境指標生物，可以用來監測海洋環境變遷、預測海洋生態系的改變。展場設\n",
            "置顯微觀察區，讓參觀者一窺這群嬌小但至關重要的小小兵身影，了解牠們是維護海洋生態平衡的關鍵多數。除了更新的三件展品外，探測基地－太空中心展出我國第一顆自製氣象衛\n",
            "星獵風者衛星（Triton）的1：1模型及其元件介紹。獵風者衛星搭載國家太空中心自行研發的「全球導航衛星系統反射訊號接收儀」（GNSS-\n",
            "R），進行海洋風場、海面高度、海氣交互作用、地表含水量、颱風強度預測等研究。智慧基地－科政中心設計出一套多媒體互動遊戲，有三題選擇題，題目出自「PRIDE政策研\n",
            "究指標資料庫」，觀眾只要揮動手臂就可以在螢幕上選擇答案，在答題的過程中對我們的世界現況建立基本認識。智慧基地－國網中心展出「搜救犬日常訓練成果三維虛擬導覽系統」\n",
            "，展現搜救犬中心的全方位實境景觀，可作為訓練領犬員及搜救犬之參考，也讓民眾認識犬隻訓練過程和了解搜救任務。國研院近年來積極把國家級實驗研究單位開發的尖端科技，轉\n",
            "化為讓中小學生都能有基本認識的科普展覽與活動，希望能幫助中小學生建立對科學的興趣，並培養科學素養。「科學家的秘密基地」會定期進行展品更新，持續提供創新的國家研究\n",
            "成果，歡迎民眾來科教館8樓探索尖端科技研究的有趣之處。\n",
            "\n",
            "2. 【國研院說明稿】關於小油坑失火事件  若鑑定結果為國研院設備造成 國研院絕不卸責\n",
            "   對於小油坑失火事件，陽明山國家公園管理處表示，初步研判此次火災有可能是國家實驗研究院國家高速網路與計算中心（國研院國網中心）之設備造成，國研院院長蔡宏營對此表示\n",
            "，國研院將慎重面對，如果最後調查結果確實是國研院國網中心之設備造成，國研院將承擔相關責任，絕不卸責。國研院已啟動內部調查，並將全力配合消防單位之外部調查，釐清起\n",
            "火原因；同時亦將仔細盤點現有設備，預防類似事件再度發生。國研院表示，國研院國網中心在小油坑地區安裝空氣品質感測器，並透過用於防災的無線電波段Band\n",
            "20，將感測資料傳輸回國網中心主機，驗證Band 20在沒有商用無線網路5G、6G及惡劣環境中，亦可傳輸資料，對於提升防救災工作的通訊傳輸能力有很大幫助；同時藉\n",
            "由空氣品質感測器，收集陽明山區域過去1年8個月的PM 2.5資料，在民生公共物聯網上對外公開，提供各界使用，幫助學研界增加對陽明山地區空氣污染情形的了解。\n",
            "\n",
            "3. 【國研院說明稿】關於小油坑失火事件\n",
            "   為偵測大屯火山活動噴發氣體與空氣品質，國家實驗研究院國家高速網路與計算中心（國研院國網中心）利用空氣品質感測器，在小油坑地區蒐集大屯山地區的空氣品質資料。\n",
            "小油坑地區位於陽明山國家公園管理範圍內，本計畫於2022年正式向陽明山國家公園管理處提出申請，並獲得同意，程序合法合規，且已於2024年底結束。\n",
            "本計畫係委託廠商架設空氣品質感測器，透過網路線即時回傳空品數據。電力來源是透過太陽能供電系統，保障在偏遠無電區域仍可持續運作。\n",
            "承包廠商以太陽能桿提供空氣品質感測器所需電力，下圖即為太陽能板發電與儲電設備及空氣品質感測器。太陽能桿與空品感測器皆為合法合規之設備，並經測試與驗證後才進行部署\n",
            "。\n",
            "\n",
            "4. 臺法電子資訊與器官晶片深化交流 合作提升雙方科技研發量能\n",
            "   國科會轄下之國家實驗研究院（國研院）與法國原子能暨替代能源總署（CEA）轄下之電子暨資訊技術研究室（CEA-Leti），於臺灣時間4月10日簽訂合作備忘錄，未來\n",
            "將以雙邊工作坊深化技術交流。法國國家健康與醫學研究院（Inserm）則與國研院於4月7日在巴黎辦理「臺法雙邊器官晶片科學論壇」，期能結合法國在基礎醫學實驗與我國\n",
            "在半導體、儀器技術的專長，建立實質的合作研究計畫，並將我國的生物晶片技術推展至歐洲。法國電子暨資訊技術研究室是法國最重要的電子資訊研究單位，專精於微電子和奈米技\n",
            "術應用。國研院與法國電子暨資訊技術研究室未來將在半導體、生醫、光學技術等領域共同舉辦工作坊或研討會，並進行人才交流互訪，期能促進雙方實質技術交流與研究合作。法國\n",
            "國家健康與醫學研究院則是法國首屈一指的生醫研究單位，此次與國研院邀集臺法雙方各12個具藥物開發應用潛力、且已完成概念性驗證或雛型品開發之研究團隊，共同辦理「臺法\n",
            "雙邊器官晶片科學論壇」，就生物材料（Biomaterial）、感測器與影像（Sensor and Imaging）及多功能器官晶片（Multicomponent\n",
            "& Multifunctional OoC）三大主題交流討論，期望透過成果媒合，促成合作研究的機會，加成雙方研發優勢。器官晶片是國科會與法國高等教育暨研究部於2\n",
            "024年臺法科學研究會議中列出的合作項目之一，是全球最受矚目的活體動物替代方法，結合了3D立體細胞培養、微流道、生醫感測器及人工智慧等技術，在實驗室重建器官內部\n",
            "的結構與環境。若模擬人體健康器官功能進行試藥，可用於藥物毒性或副作用的測試；若模擬生病的器官，則有助於藥物功效的測試與篩選；若能建立病人的器官晶片，更有機會直接\n",
            "運用於個人化醫療策略的輔助評估。此次參與論壇的臺灣團隊來自國科會生科處專案補助之「動物實驗替代科技研發計畫」，以及國研院的「器官晶片多元驗證平台」，包括陽明交通\n",
            "大學、清華大學、臺北醫學大學、國家衛生研究院、長庚醫院，以及國研院國家生物模式中心（原國家實驗動物中心）與國家儀器科技研究中心（原台灣儀器科技研究中心）的研究團\n",
            "隊。論壇結束後，國研院蔡宏營院長並率隊參訪法國巴黎－薩克雷大學（UPSaclay）及法國國家科學研究院（CNRS），期能建立臺法雙方更緊密、更全面的合作關係，藉\n",
            "由實習生交流、大型研究設施共享、合作執行研究計畫等，提升臺法雙方在生醫、高速計算、儀器科技領域的科技研發量能。\n",
            "\n",
            "5. 國立陽明交通大學、振興醫院與國研院國儀中心 共同開發新型微米級光纖麥克風 為聽障者提供更無礙的聲音感受\n",
            "   為使聽障者享有更無礙的聲音感受，國立陽明交通大學、振興醫院與國家實驗研究院國家儀器科技研究中心（國研院國儀中心）合作，成功開發出「微型法布里–珀羅光纖光學麥克風\n",
            "」（Miniaturized Fabry-Perot fiber-optic microphone based on capillary tube and\n",
            "hydrogel diaphragm），解決麥克風受磁場干擾之問題，其研究成果榮登光學領域頂級期刊《Optics & Laser Technology》。此新型\n",
            "微米級光纖麥克風結構簡單、成本低、訊號穩定、尺寸微小，整個麥克風的尺寸如同一根頭髮，而靈敏度比現有技術提升了約37%，能更細微地捕捉聲音變化，同時可以偵測更高頻\n",
            "的聲音，並且兼顧了輕薄微小的功能，未來可應用於穿戴式裝置上。\n",
            "麥克風是一種將聲音轉換成電子訊號的換能器，在生物醫學影像、語音互動系統和助聽器等應用中發揮著至關重要的作用。然而一般麥克風容易受到強電磁場或是射頻干擾，也有電噪\n",
            "聲過大、靈敏度較低的問題。此次發表之新型微米級光纖麥克風，是利用光纖與光學元件來檢測聲波引起的變化，並將這些變化轉換為可測量的光信號。當聲音壓力作用在麥克風的膜\n",
            "片上，造成膜片的變形或振動，改變了光的干涉條紋，藉由此變化轉換為電子信號來偵測聲音。國立陽明交通大學生物醫學工程學系劉承揚教授、振興醫院耳鼻喉部力博宏主任醫師與\n",
            "國研院國儀中心協力合作，首先由力博宏主任醫師發想光纖麥克風的構造概念，接著由國儀中心協助薄膜（麥克風的膜片）製程與檢測，並由劉承揚教授團隊將光纖與薄膜結合，再進\n",
            "行測試與臨床驗證，最終成功開發出新型微米級光纖麥克風。整個麥克風的尺寸如同一根頭髮，同時具有優異的聲音感測靈敏度和穩定性。新型微米級光纖麥克風由於不含金屬材料，\n",
            "因此不受電磁干擾，適用於助聽器和人工電子耳，使用者可以自由進入強電磁場環境而不必擔心噪音的產生，在光聲成像、健康監測、無損檢測、醫學臨床等應用方面亦有具有巨大的\n",
            "商業潛力。\n",
            "國研院國儀中心薄膜製程開發技術之前亦曾協助劉承揚教授團隊成功用蜘蛛絲製成光纖感測器，精準量測糖尿病患者血糖，近期更往高性能半導體材料應用發展。此次協助光纖薄膜製\n",
            "作，再次證明國儀中心是學術界挑戰世界頂尖科技的關鍵夥伴。國儀中心也期待與國內更多學者合作，應用頂尖的薄膜製程開發技術，研發出更多尖端生醫光電醫療器材。\n",
            "\n",
            "6. TAIWAN AI RAP試營運啟動 徵求試營運用戶，免費算力助攻AI開發！\n",
            "   為加速臺灣生成式AI之應用發展，幫助百工百業創造符合其需求的專屬AI，國科會轄下國家實驗研究院國家高速網路與計算中心（國研院國網中心）推出「TAIWAN AI \n",
            "RAP1」高效能AI應用開發平台，並正式啟動試營運計畫，即日起開放資訊服務業者、新創公司、中小企業、學研單位提出應用主題開發提案，通過評選者將可獲得免費GPU算\n",
            "力及全方位AI開發資源支持，加速AI產品與應用落地。TAIWAN AI RAP是國網中心打造的高效能AI平台，整合強大算力、生成式AI工具及標準化開發流程，提供\n",
            "從算力支援、AI開發工具到應用部署的完整解決方案，以B2B模式提供靈活、高效的AI應用開發環境，協助實現多元AI應用概念，縮短生成式AI應用開發及服務驗證時間，\n",
            "進而創造符合各行各業需求的專屬AI。國研院國網中心張朝亮主任表示，各種行業都在加速引進AI，透過TAIWAN AI RAP，可降低引進AI的技術門檻，使用者無須\n",
            "自行負擔昂貴的AI訓練設備建置及維運成本，並享有AI工具、開發框架與技術資源的完整支援。企業與研究單位可更專注於創新應用開發，提升AI研發效能或加快AI產品市場\n",
            "化進程。試營運申請分兩階段，第一階段申請時間為即日起至4月24日，第二階段為7月7日至7月16日，採申請審核制。申請者需提出開發AI產品或服務之規劃，國研院國網\n",
            "中心將優先評選擁有清晰AI開發願景，及對AI應用開發、模型訓練或AI服務普及化有高度企圖心的團隊。通過評選者將獲得免費GPU算力，換算約等值國網中心iServi\n",
            "ce服務網上新臺幣65萬元的算力額度。此外，國研院國網中心將於4月18日舉辦試營運說明會，除介紹如何參加徵選，以獲得免費GPU算力及全方位AI開發資源支持，並將\n",
            "邀請優秀的測試先行者，分享他們透過TAIWAN AI RAP實現AI應用的案例，幫助有意願開發專屬AI的團隊深入了解如何透過TAIWAN AI\n",
            "RAP快速開發AI產品。歡迎符合資格的企業與單位把握機會，共同推動臺灣AI應用發展。\n",
            "➽客製化流程設計前台整合AI工具與工作流程，提供低門檻、無技術背景也能使用的彈性開發環境，支援快速部署與介面設計，降低開發門檻，提升應用落地效率。\n",
            "➽多模型API服務內建多種開源模型，包含TAIDE系列、Llama系列與Phi系列等，強化臺灣繁體中文應用導入，提供迅速開發加值服務，滿足各行業AI需求。\n",
            "➽模型微調與評估簡化AI訓練與數據處理，提升模型的準確性與適用性。例如，零售業可透過微調技術打造符合品牌風格的AI購物顧問，優化顧客體驗並提升轉化率。\n",
            "➽AI Tribe聚落鼓勵國內AI服務開發者將自家AI工具掛載至TAIWAN AI RAP，進行POC驗證，形成多元AI聚落，透過共享技術與應用共創，促進產業創\n",
            "新與協同發展。4月18日試營運用戶徵選說明會報名：https://reurl.cc/9DO1jO申請方式與詳細資訊，請見官方網站：https://rap.gen\n",
            "ai.nchc.org.tw/poc\n",
            "\n",
            "7. 114年度「公民團體創新示範與沙盒試驗計畫」 徵件說明會\n",
            "   由國家實驗研究院執行國科會計畫所成立的「臺灣淨零科技方案推動小組」（簡稱淨零推動小組），推動「公民團體創新示範與沙盒試驗計畫」（簡稱沙盒計畫），補助民間團體進行\n",
            "創新研發與實作驗證。今（3/28）日舉辦徵件說明會，希望邀請關心永續議題的公民團體、社區組織、新創團隊等，踴躍參與114年度沙盒計畫。\n",
            "沙盒計畫以結合科技力、社會力與網路力為執行目標，徵求民間淨零解方之試驗場域（Test bed）與生活實驗室（Living Lab），透過自主提案方式，鼓勵第三部\n",
            "門（非營利與非政府組織）發展淨零公民科技與普惠科技，共創淨零創新生態系。沙盒計畫已邁入第三期，奠基於過去兩期計畫之推動成果，共計已有35組試點團隊，北、中、南、\n",
            "東及城、鄉均有代表性團隊執行試驗主題並嘗試研擬創新解方，主題涵蓋永續能源、循環經濟、自然碳匯、低碳農業、低碳產業及淨零綠生活等。\n",
            "淨零推動小組林耀東副主任以113年度沙盒計畫參與者之一的「桔屋－集吉屋」細分類資源回收站為例指出，這個計畫是由主婦聯盟環境保護基金會臺中分會攜手公老坪產業發展協\n",
            "會、大豐環保、逢甲大學建築社造創生研究室等單位合作推動，團隊將社區參與、環境行動與經濟循環緊密結合，充分展現社區自主驅動淨零轉型的力量。這不只是建置一座回收站，\n",
            "更是社區透過共學、共作、共好的方式，落實「細分類、全回收、零廢棄」的生活轉型，展現社區自主實驗與政策協力共構的淨零行動樣貌。經初步估計，公老坪社區所有居民落實資\n",
            "源回收進行細分類，每年將可減少5,469公斤的溫室氣體排放，若公老坪社區沙盒試點單位之減碳典範擴散至全國6,823個社區發展協會，推估每年約可減少37,526,\n",
            "500公斤的溫室氣體排放。依據沙盒計畫歷年成果顯示，由下而上之社區自主創新實驗，對台灣淨零目標具有重大貢獻。淨零推動小組周素卿首席顧問表示，在臺灣邁向2050淨\n",
            "零的進程中，類似「桔屋－集吉屋」細分類資源回收站這樣的社區創新行動，不僅補足政策執行之空缺，亦提供具觀測性與可複製之治理創新案例。淨零推動小組未來將持續以沙盒計\n",
            "畫為平台，支持更多社區勇於嘗試，共創淨零轉型路徑。114年度「公民團體創新示範與沙盒試驗計畫」徵件遴選作業相關資訊，公告於國家實驗研究院官網（https://r\n",
            "eurl.cc/9Dz8On），申請期限為4月17日。\n",
            "\n",
            "8. 國家實驗研究院英文名稱更名揭牌\n",
            "   國科會轄下之財團法人國家實驗研究院（國研院）今（3/25）日宣布變更英文名稱，由原本的「National Applied Research\n",
            "Laboratories」（簡稱「NARLabs」），變更為「National Institutes of Applied Research」（簡稱「NIAR」\n",
            "，音同near），並且舉行揭牌儀式，在國研院董監事們的見證下，由國科會主委兼國研院董事長吳誠文與國研院院長蔡宏營共同揭牌。國研院成立於2003年，一開始是由原隸\n",
            "屬於國科會的六個國家實驗室組成，分別為國家實驗動物中心、國家地震工程研究中心、國家高速網路與計算中心、國家太空計畫室（2023年改制為行政法人國家太空中心）、國\n",
            "家奈米元件實驗室及國家晶片系統設計中心（二者於2019年整併為台灣半導體研究中心）。因為是由眾多國家實驗室組成，故命名為「國家實驗研究院」，英文名稱為「Nati\n",
            "onal Applied Research Laboratories」。歷經22年，國研院目前下轄國家實驗動物中心、國家地震工程研究中心、國家高速網路與計算中心\n",
            "、台灣半導體研究中心、科技政策研究與資訊中心、台灣儀器科技研究中心與台灣海洋科技研究中心等七大研究中心。隨著國研院扮演國家科技推動的角色日趨重要，業務範圍和合作\n",
            "對象不斷擴大，與全球頂尖研究機構的合作也日益頻繁，為了可一致性地呈現國研院的專業形象，提升大眾對國研院的認知，因此更新英文名稱為「National\n",
            "Institutes of Applied Research」。國研院亦同時更新企業識別系統，使用新的英文簡稱「NIAR」（音同near），代表國研院融合多元科\n",
            "技、突破科技界限、搭建產學橋梁、促成跨域合作、迎向世界、密切與國際接軌的精神。新的企業識別系統融合碎形、拱柱、方尖碑和圓融弧度等元素，旨在象徵國研院的深度與穩固\n",
            "性。碎形AR代表無限探索與不斷創新以及無限的開放性；拱柱I與方尖碑A則體現了結構的穩固與力量，代表科學研究的基石，向上的型態也意味著邁向科技領先；而R的圓融弧度\n",
            "則象徵著和諧與整體性，代表國研院在產官學界扮演居中協調與介接的角色，用溫和的方式推動前行。色彩設定則是沿用國研院原本的色系，採用金色的厚重感搭配正紅色，讓品牌色\n",
            "調看起來沉穩而有自信。\n",
            "此外，國研院轄下之國家實驗動物中心也同時更名為國家生物模式中心，英文名稱變更為「National Center for\n",
            "Biomodels」（簡稱「NCB」）；台灣儀器科技研究中心更名為國家儀器科技研究中心，英文名稱變更為「National Center for\n",
            "Instrumentation Research」（簡稱「NCIR」）。\n",
            "\n",
            "9. 國網中心晶創主機Nano 5徵案啟動 南台灣半導體業搶搭高效能運算列車\n",
            "   為推動南部地區半導體業者數位創新與技術升級，協助企業開發更多元且具產業競爭力的系統應用技術，打造半導體產業鏈的高潛力創新解決方案，國科會轄下國家實驗研究院國家高\n",
            "速網路與計算中心（國研院國網中心）日前在臺南舉辦「晶創主機Nano 5半導體產業創新與升級」徵案說明會，吸引眾多半導體領域企業與新創團隊參與。「晶創主機Nano\n",
            "5」為新一代加速器叢集式超級電腦，作為發展台灣AI應用及高效能運算的關鍵基礎設施。晶創主機承襲「晶片驅動臺灣產業創新方案」的願景，助力半導體產業發展AI應用，也\n",
            "為半導體產業其他應用及研發，如加速晶片設計與驗證、AI與半導體技術整合、半導體製造數據分析、先進封裝與異質整合技術、晶圓製造與智慧工廠應用等，提供強大且高速的計\n",
            "算環境。該系統採用NVIDIA H100 GPU計算加速器，總算力將達16 PetaFLOPS1，可大幅提升人工智慧模型訓練與科學模擬的運算效率。國研院國網中心\n",
            "副主任姚志民表示，全球半導體產業持續升級，智慧化趨勢快速發展，高效能運算已成為驅動技術創新與產業升級的核心動能。晶創主機Nano 5代表臺灣半導體產業的實力與創\n",
            "新能量，從微小到強大的完美結合。國網中心希望透過此次徵案，為南部地區半導體業者提供專屬的高效能運算資源，支持企業突破技術瓶頸，進而提升整體產業競爭力。此活動同時\n",
            "邀請多位業界專家剖析產業趨勢與成功案例，DIGITIMES顧問分析師兼領域總監黃銘章以「算力經濟時代下半導體產業的發展趨勢」為題發表專題演講，深入分析在AI浪潮\n",
            "下，半導體產業如何掌握新興商機與應對挑戰；群聯電子技術長林緯分享「企業AI轉型關鍵：本土化訓練與在地運算的未來」，探討企業數位轉型實務經驗與成功案例；國網中心研\n",
            "究員李玟頡則介紹「以AI高效能運算加速半導體元件設計與創新應用」，展示高效能運算如何有效提升研發效率與創新能量。國研院國網中心此次徵案側重三大核心價值，首先是提\n",
            "供高效能運算服務環境，幫助企業在數位創新中快速突破；其次是推動技術升級，全力支持開發多元且具競爭力的系統應用，提升半導體產業鏈的核心技術；最後為強化產業鏈韌性，\n",
            "透過高效能運算技術，提升產業鏈的應變能力與市場競爭力。國研院國網中心表示，本次徵案即日起開放申請，4月2日截止，歡迎各企業與新創公司踴躍參與，共同推動臺灣半導體\n",
            "技術邁向新高度。有意參與徵案的企業與團隊，可至國網中心官網（https://www.nchc.org.tw/）查詢詳細資訊，或直接聯繫計畫專案小組（jay.ch\n",
            "en@digitimes.com）諮詢相關申請流程與資格條件。\n",
            "\n",
            "10. 國研院x國資圖 科學家的秘密基地@臺中 「地震工程大解密－讓建築更安全的秘密」地震科普展 房屋耐震，大家一起來！\n",
            "   臺灣地震頻仍，地震災情時有所聞，政府因而投注相當多資源進行災防科技相關研究，並藉由科普展覽與活動，幫助民眾學習基本災防知識，落實正確的災防觀念。國科會轄下國家實\n",
            "驗研究院國家地震工程研究中心（國研院國震中心）與國立公共資訊圖書館（國資圖）合作，於國資圖2樓「科學家的秘密基地@臺中」辦理「地震工程大解密－讓建築更安全的秘密\n",
            "」地震工程科普展，昨（3/18）日開展，介紹基本地震工程知識，使民眾了解讓建築耐震安全的秘密。2024年4月3日在花蓮發生芮氏規模7.1的強震，是繼1999年9\n",
            "21大地震後，規模最大的地震，在花東地區造成嚴重災損，多人罹難及受傷，並有多棟房屋倒塌或半倒，許多道路、橋梁、維生管線、學校等基礎設施受到不同程度的損毀。202\n",
            "5年1月21日於臺南市楠西區又發生芮氏規模6.4的強震，同樣造成許多災損，再再都提醒人們地震的可怕。地震是臺灣的宿命，沒有地震就沒有臺灣島的形成。如何和地震共存\n",
            "，是居住在臺灣所有人必須共同面臨的課題。\n",
            "因此，建築安全向來是臺灣社會關注的焦點。本次特展共分三個部分，包含「地震工程大解密」主題故事報架、傾斜柱拍照區以及先進減震技術介紹等。「地震工程大解密」主題故事\n",
            "書報架內含10篇地震工程專題報導，全面解析地震工程科技，揭露如何透過耐震、減震、隔震技術與建築補強工法，提升建築物在地震中的安全性。透過科學研究與工程技術的結合\n",
            "，幫助臺灣的建築朝向更安全、更可靠的方向邁進。傾斜柱拍照區則配合現場的傾斜柱，模擬經強烈地震震損後的房屋樣貌，搭配地貼等裝飾，讓現場民眾體驗位於受災現場的震撼。\n",
            "先進減震技術運作原理則是透過國震中心研發的「挫屈束制支撐」縮尺模型，以及手搖振動模型展示，說明該技術如何藉由設置在建築物樓層間吸收地震能量，進而大幅降低地震破壞\n",
            "，提升房屋的耐震性，讓參觀民眾可以更清楚知道減震結構與傳統建築的差異。\n",
            "臺灣位處環太平洋地震帶，地震風險極高，建築耐震技術的發展與應用至關重要。本次特展透過深入淺出的科學解釋，幫助民眾理解地震工程的核心技術，從耐震設計、減震隔震技術\n",
            "到建築補強方案，全面提升建築安全性。隨著科技的進步與法規的完善，未來的臺灣建築將更具抗震韌性，為全民提供更安全的居住環境。最後，本次展覽設有耐震紙模型屋DIY區\n",
            "，現場提供紙模型材料，不管大人小孩都可以自己動手搭建耐震小紙屋，了解牆壁對於房屋耐震的重要性。透過科普書報、拍照打卡、模型展示與實際動手做，讓所有到訪民眾對於房\n",
            "屋耐震安全能有更深一層的認識，大家一起來關心並改善自家耐震安全。\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import textwrap\n",
        "from collections import Counter\n",
        "\n",
        "base_url = 'https://www.niar.org.tw'\n",
        "news_base_url = 'https://www.niar.org.tw/xmdoc?xsmsid=0I148622737263495777'\n",
        "\n",
        "page = 1\n",
        "news_count = 0\n",
        "\n",
        "special_terms = [\n",
        "    \"國家生物模式中心\",\n",
        "    \"國家地震工程研究中心\",\n",
        "    \"國家高速網路與計算中心\",\n",
        "    \"台灣半導體研究中心\",\n",
        "    \"國家儀器科技研究中心\",\n",
        "    \"科技政策研究與資訊中心\",\n",
        "    \"台灣海洋科技研究中心\"\n",
        "]\n",
        "\n",
        "print(\"國家實驗研究院新聞列表：\\n\")\n",
        "\n",
        "while news_count<10:\n",
        "    url = f\"{news_base_url}&page={page}\"\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    rows = soup.find_all('tr')\n",
        "\n",
        "    found_news = False\n",
        "\n",
        "    for row in rows:\n",
        "        date_td = row.find('td', class_='date')\n",
        "        title_td = row.find('td', class_='title')\n",
        "\n",
        "        if date_td and title_td:\n",
        "            found_news = True\n",
        "            news_count += 1\n",
        "            #date = date_td.text.strip()\n",
        "            a_tag = title_td.find('a')\n",
        "            title = a_tag.text.strip()\n",
        "            '''tags = tagger.tag(title)  # 這裡可能是 list[list[tuple]]\n",
        "            filtered = [filter_nouns_verbs(sent) for sent in tags]\n",
        "            words = [word for sent in filtered for word, pos in sent]\n",
        "            unique = unique_words(words)'''\n",
        "            link = base_url + a_tag['href']\n",
        "            article_response = requests.get(link)\n",
        "            article_response.encoding = 'utf-8'\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "            # 抓取文章內容\n",
        "            article_response = requests.get(link)\n",
        "            article_response.encoding = 'utf-8'\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "            content = article_soup.find_all('p', class_='MsoNormal')\n",
        "            content_text = \"\\n\".join(p.get_text(strip=True)  for p in content  if p.get_text(strip=True))\n",
        "\n",
        "            found_special_terms = [term for term in special_terms if term in content_text]\n",
        "\n",
        "            tags = tagger.tag(content_text)\n",
        "            filtered = [filter_nouns_verbs(sent) for sent in tags]\n",
        "            words = [word for sent in filtered for word, pos in sent]\n",
        "\n",
        "            for term in found_special_terms:\n",
        "              for subword in term:\n",
        "                  words = [word for word in words if word != subword]\n",
        "\n",
        "            words += found_special_terms\n",
        "\n",
        "            counter = Counter(words)\n",
        "            top_10 = counter.most_common(10)\n",
        "            print(f\"{news_count}. {title}\\n   {top_10}\\n   {found_special_terms}\\n\")\n",
        "\n",
        "            #print(f\"{news_count}. {title}\\n   {content_text}\\n\")\n",
        "            #print(f\"{news_count}. [{date}] {title}\\n   {tags}\\n   {unique}\\n   👉 {link}\\n\")\n",
        "\n",
        "    if not found_news:\n",
        "        break  # 沒有新內容就停止\n",
        "    page += 1\n"
      ],
      "metadata": {
        "id": "fDthf2FObAoo",
        "outputId": "f4f18b48-f709-4cee-97ae-fd614b139c78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "國家實驗研究院新聞列表：\n",
            "\n",
            "1. 科教館「科學家的秘密基地」 4/16更新展區重新亮相\n",
            "   [('中心', 17), ('海洋', 13), ('基地', 12), ('研究', 8), ('國家', 7), ('動物', 7), ('實驗', 6), ('參觀', 6), ('基因', 6), ('者', 6)]\n",
            "   ['國家生物模式中心', '國家地震工程研究中心', '台灣海洋科技研究中心']\n",
            "\n",
            "2. 【國研院說明稿】關於小油坑失火事件  若鑑定結果為國研院設備造成 國研院絕不卸責\n",
            "   [('國研院', 8), ('中心', 5), ('陽明山', 3), ('國家', 3), ('表示', 3), ('國網', 3), ('設備', 3), ('調查', 3), ('空氣', 3), ('資料', 3)]\n",
            "   ['國家高速網路與計算中心']\n",
            "\n",
            "3. 【國研院說明稿】關於小油坑失火事件\n",
            "   [('空氣', 6), ('品質', 6), ('感測器', 5), ('國家', 4), ('地區', 3), ('中心', 2), ('小油坑', 2), ('陽明山', 2), ('公園', 2), ('本', 2)]\n",
            "   ['國家高速網路與計算中心']\n",
            "\n",
            "4. 臺法電子資訊與器官晶片深化交流 合作提升雙方科技研發量能\n",
            "   [('研究', 12), ('法國', 11), ('技術', 10), ('器官', 9), ('國家', 8), ('國研院', 7), ('合作', 7), ('晶片', 7), ('研究院', 5), ('交流', 5)]\n",
            "   ['國家生物模式中心', '國家儀器科技研究中心']\n",
            "\n",
            "5. 國立陽明交通大學、振興醫院與國研院國儀中心 共同開發新型微米級光纖麥克風 為聽障者提供更無礙的聲音感受\n",
            "   [('麥克風', 13), ('光纖', 10), ('聲音', 7), ('中心', 7), ('國儀', 6), ('應用', 5), ('薄膜', 5), ('此', 4), ('新型', 4), ('微米級', 4)]\n",
            "   ['國家儀器科技研究中心']\n",
            "\n",
            "6. TAIWAN AI RAP試營運啟動 徵求試營運用戶，免費算力助攻AI開發！\n",
            "   [('開發', 19), ('應用', 15), ('中心', 7), ('服務', 7), ('國網', 6), ('者', 6), ('工具', 5), ('技術', 5), ('申請', 5), ('模型', 5)]\n",
            "   ['國家高速網路與計算中心']\n",
            "\n",
            "7. 114年度「公民團體創新示範與沙盒試驗計畫」 徵件說明會\n",
            "   [('淨零', 13), ('計畫', 12), ('社區', 11), ('沙盒', 10), ('推動', 8), ('創新', 7), ('小組', 5), ('實驗', 4), ('執行', 4), ('公民', 4)]\n",
            "   []\n",
            "\n",
            "8. 國家實驗研究院英文名稱更名揭牌\n",
            "   [('國家', 18), ('中心', 18), ('國研院', 15), ('研究', 12), ('科技', 9), ('英文', 6), ('實驗', 5), ('名稱', 5), ('簡稱', 5), ('台灣', 5)]\n",
            "   ['國家生物模式中心', '國家地震工程研究中心', '國家高速網路與計算中心', '台灣半導體研究中心', '國家儀器科技研究中心', '科技政策研究與資訊中心', '台灣海洋科技研究中心']\n",
            "\n",
            "9. 國網中心晶創主機Nano 5徵案啟動 南台灣半導體業搶搭高效能運算列車\n",
            "   [('半導體', 14), ('產業', 12), ('技術', 10), ('創新', 9), ('運算', 9), ('企業', 8), ('中心', 8), ('應用', 7), ('國網', 7), ('高效能', 7)]\n",
            "   ['國家高速網路與計算中心']\n",
            "\n",
            "10. 國研院x國資圖 科學家的秘密基地@臺中 「地震工程大解密－讓建築更安全的秘密」地震科普展 房屋耐震，大家一起來！\n",
            "   [('地震', 20), ('工程', 10), ('建築', 10), ('耐震', 8), ('技術', 8), ('臺灣', 7), ('安全', 7), ('民眾', 6), ('讓', 5), ('房屋', 5)]\n",
            "   ['國家地震工程研究中心']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YaGGiYvYcTHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ckiptagger"
      ],
      "metadata": {
        "id": "25VMyybXPVGN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikdo30qW9rvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ca2c7d-a1a5-446c-a302-c75e284ae91b"
      },
      "source": [
        "pip install -U ckiptagger[tf,gdown]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ckiptagger[gdown,tf]\n",
            "  Downloading ckiptagger-0.2.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: tensorflow>=1.13.1 in /usr/local/lib/python3.9/dist-packages (from ckiptagger[gdown,tf]) (2.12.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (from ckiptagger[gdown,tf]) (4.6.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.12.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (67.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.2.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.22.4)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.14.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (23.3.3)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (23.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.3.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.4.7)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.32.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.20.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (16.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown->ckiptagger[gdown,tf]) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown->ckiptagger[gdown,tf]) (3.11.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown->ckiptagger[gdown,tf]) (4.11.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown->ckiptagger[gdown,tf]) (2.27.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.0.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.10.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.17.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown->ckiptagger[gdown,tf]) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (1.7.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (6.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.2.2)\n",
            "Installing collected packages: ckiptagger\n",
            "Successfully installed ckiptagger-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ckiptagger import data_utils, construct_dictionary, WS, POS\n",
        "data_utils.download_data_gdown(\"./\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHS8fZAYVKh5",
        "outputId": "818e450c-0430-4fa2-c7d1-7ca0e62d4f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
            "To: /content/data.zip\n",
            "100%|██████████| 1.88G/1.88G [00:23<00:00, 81.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ws = WS(\"./data\")\n",
        "pos = POS(\"./data\")"
      ],
      "metadata": {
        "id": "S3JFsiu4VbNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b86c086-e4f8-41be-a8ec-e581a58ca18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDTcqgmo9ryM"
      },
      "source": [
        "sentence_list = [\n",
        "    \"綜合生魚片中的旗魚非常乾，有點難以下嚥，鮪魚有一點腥，其餘皆尚可，炒松阪肉味道調很好，肉質Q彈有嚼勁，蝦子給的算蠻豐盛蠻大隻的，但吃起來沒有很甜，肉有點老，炸白身魚外酥內軟，還有炸米餅，口感層次很多，很推一的一道菜\",\n",
        "    \"點了皇家牛來的時候外觀真的是下一位。肥油沒處理筋一堆真的是不知道在吃什麼很反胃\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sentence_list = ws(sentence_list)"
      ],
      "metadata": {
        "id": "Kn2lmRp0vrDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sentence_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxRNlu8yvrGA",
        "outputId": "e09e4b68-c5b1-4150-c507-3c5b4b8e1887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['綜合',\n",
              "  '生魚片',\n",
              "  '中',\n",
              "  '的',\n",
              "  '旗魚',\n",
              "  '非常',\n",
              "  '乾',\n",
              "  '，',\n",
              "  '有點',\n",
              "  '難以',\n",
              "  '下嚥',\n",
              "  '，',\n",
              "  '鮪魚',\n",
              "  '有',\n",
              "  '一點',\n",
              "  '腥',\n",
              "  '，',\n",
              "  '其餘',\n",
              "  '皆',\n",
              "  '尚可',\n",
              "  '，',\n",
              "  '炒',\n",
              "  '松阪肉',\n",
              "  '味道',\n",
              "  '調',\n",
              "  '很',\n",
              "  '好',\n",
              "  '，',\n",
              "  '肉質',\n",
              "  'Q彈',\n",
              "  '有',\n",
              "  '嚼勁',\n",
              "  '，',\n",
              "  '蝦子',\n",
              "  '給',\n",
              "  '的',\n",
              "  '算',\n",
              "  '蠻',\n",
              "  '豐盛',\n",
              "  '蠻',\n",
              "  '大',\n",
              "  '隻',\n",
              "  '的',\n",
              "  '，',\n",
              "  '但',\n",
              "  '吃起來',\n",
              "  '沒有',\n",
              "  '很',\n",
              "  '甜',\n",
              "  '，',\n",
              "  '肉',\n",
              "  '有點',\n",
              "  '老',\n",
              "  '，',\n",
              "  '炸',\n",
              "  '白身魚',\n",
              "  '外酥',\n",
              "  '內',\n",
              "  '軟',\n",
              "  '，',\n",
              "  '還',\n",
              "  '有',\n",
              "  '炸米餅',\n",
              "  '，',\n",
              "  '口感',\n",
              "  '層次',\n",
              "  '很多',\n",
              "  '，',\n",
              "  '很',\n",
              "  '推',\n",
              "  '一',\n",
              "  '的',\n",
              "  '一',\n",
              "  '道',\n",
              "  '菜'],\n",
              " ['點',\n",
              "  '了',\n",
              "  '皇家',\n",
              "  '牛',\n",
              "  '來',\n",
              "  '的',\n",
              "  '時候',\n",
              "  '外觀',\n",
              "  '真的',\n",
              "  '是',\n",
              "  '下',\n",
              "  '一',\n",
              "  '位',\n",
              "  '。',\n",
              "  '肥油',\n",
              "  '沒',\n",
              "  '處理',\n",
              "  '筋',\n",
              "  '一',\n",
              "  '堆',\n",
              "  '真的',\n",
              "  '是',\n",
              "  '不',\n",
              "  '知道',\n",
              "  '在',\n",
              "  '吃',\n",
              "  '什麼',\n",
              "  '很',\n",
              "  '反胃']]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_sentence_list = pos(word_sentence_list)\n",
        "pos_sentence_list"
      ],
      "metadata": {
        "id": "DHrAj8UOwGEm",
        "outputId": "be4a7f2c-6dcb-4ba6-add1-594ee925c19f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A',\n",
              "  'Na',\n",
              "  'Ng',\n",
              "  'DE',\n",
              "  'Na',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Dfa',\n",
              "  'D',\n",
              "  'VA',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'V_2',\n",
              "  'Neqa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Neqa',\n",
              "  'D',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'VC',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'V_2',\n",
              "  'Na',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'VD',\n",
              "  'DE',\n",
              "  'VG',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'Nf',\n",
              "  'T',\n",
              "  'COMMACATEGORY',\n",
              "  'Cbb',\n",
              "  'D',\n",
              "  'D',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'VC',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'Ncd',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'D',\n",
              "  'V_2',\n",
              "  'Na',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Dfa',\n",
              "  'VC',\n",
              "  'Neu',\n",
              "  'DE',\n",
              "  'Neu',\n",
              "  'Nf',\n",
              "  'Na'],\n",
              " ['VC',\n",
              "  'Di',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'VA',\n",
              "  'DE',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'D',\n",
              "  'SHI',\n",
              "  'Nes',\n",
              "  'Neu',\n",
              "  'Nf',\n",
              "  'PERIODCATEGORY',\n",
              "  'Na',\n",
              "  'D',\n",
              "  'VC',\n",
              "  'Na',\n",
              "  'Neu',\n",
              "  'Nf',\n",
              "  'D',\n",
              "  'SHI',\n",
              "  'D',\n",
              "  'VK',\n",
              "  'D',\n",
              "  'VC',\n",
              "  'Nep',\n",
              "  'Dfa',\n",
              "  'VH']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRoErrZVwGMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCtA99OQydVv"
      },
      "source": [
        "# https://www.nltk.org/\n",
        "import nltk\n",
        "import json\n",
        "from google.colab import files # 在 Google Colab 環境上傳檔案時所用；若在個人電腦執行 Python 則不需要\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afVpBzzya7Uu"
      },
      "source": [
        "# 若下載 python 檔並在自己電腦執行，開檔讀檔請用這段 # 若要上傳檔案到 Google Colab 虛擬主機，請註解掉這段\n",
        "#f = open(\"D:/cjlin/sample/ReutersCorn-sample10.json\", \"r\", encoding='UTF-8') # encoding='UTF-8' 才能正確讀入中文檔案\n",
        "#docText = f.read()\n",
        "#f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szej4JgszM_i"
      },
      "source": [
        "# 由電腦上傳檔案至 Colab 虛擬主機 # 若要在個人電腦單機作業，請註解掉這段\n",
        "file = files.upload()\n",
        "trf = open(\"ReutersCorn-train.json\", \"r\", encoding='UTF-8')\n",
        "fdoc = trf.read()\n",
        "trf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfG1FHwyI1aV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYkRVZhDI11E"
      },
      "source": [
        "dataset = json.loads(fdoc)\n",
        "dataset[0]['text'] # 此行僅用來確認執行結果正確，可刪去"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlHjR_iQI66q"
      },
      "source": [
        "file = files.upload()\n",
        "dictf = open(\"posDict_ReutersCorn.json\", \"r\", encoding='UTF-8')\n",
        "dict_raw = dictf.read()\n",
        "dictf.close()\n",
        "POSdict = json.loads(dict_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FccRYuKpJdj0"
      },
      "source": [
        "wordTotal = 0\n",
        "tokenSet = set()\n",
        "for doc in dataset:\n",
        "  word = nltk.word_tokenize(doc['text'])\n",
        "  wordTotal += len(word)\n",
        "  #nltk_token = []\n",
        "  for w in word:\n",
        "    tokenSet.add(w)\n",
        "    #nltk_token.append(w)\n",
        "  #doc['nltk_token'] = nltk_token\n",
        "print(wordTotal)\n",
        "len(tokenSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGBCTag6SYiY"
      },
      "source": [
        "lowerSet = set()\n",
        "for w in tokenSet:\n",
        "  lowerSet.add(w.lower())\n",
        "len(lowerSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-0kTVS0fOxl"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "d7wfjrc_VqZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sets = ['automate', 'automatic', 'automation', 'computer', 'computerize', 'computerization', 'computerizational']\n",
        "for w in word_sets:\n",
        "  stemstr = lemmatizer.lemmatize(w, \"v\")\n",
        "  print(w + ' ==> ' + stemstr)"
      ],
      "metadata": {
        "id": "P5fma8I6Vao6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acqHuXPWVay_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zh1sYiTkVa2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sus2mFI2eIAd"
      },
      "source": [
        "lemmaWOpos = set()\n",
        "for w in lowerSet:\n",
        "  lemmaWOpos.add(lemmatizer.lemmatize(w))\n",
        "len(lemmaWOpos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZdzoz17Lh0B"
      },
      "source": [
        "lemmaWpos = set()\n",
        "for w in lowerSet:\n",
        "  pos = POSdict.get(w)\n",
        "  if pos != None and pos != \"x\":\n",
        "    if pos == \"j\": pos = \"a\"\n",
        "    lemmaWpos.add(lemmatizer.lemmatize(w, pos))\n",
        "  else:\n",
        "    lemmaWpos.add(lemmatizer.lemmatize(w))\n",
        "len(lemmaWpos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWkPiqbif10E"
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BMXL0Kfcxc"
      },
      "source": [
        "stemSet = set()\n",
        "for w in lowerSet:\n",
        "  stemSet.add(stemmer.stem(w))\n",
        "len(stemSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "pxF9xIDYY7Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sets = ['automate', 'automatic', 'automation', 'computer', 'computerize', 'computerization', 'computerizational']\n",
        "for w in word_sets:\n",
        "  stemstr = stemmer.stem(w)\n",
        "  print(w + ' ==> ' + stemstr)\n"
      ],
      "metadata": {
        "id": "F1QesJ85z_te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U ckiptagger[tf,gdown]"
      ],
      "metadata": {
        "id": "FD3Lb0o0apWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER"
      ],
      "metadata": {
        "id": "kYsB82y-awlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_utils.download_data_gdown(\"./\")"
      ],
      "metadata": {
        "id": "ytrXrn2za6oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ws = WS(\"./data\")\n",
        "pos = POS(\"./data\")"
      ],
      "metadata": {
        "id": "J3Qv5BSxbGqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = [\n",
        "    \"土地公有政策?？還是土地婆有政策。.\",\n",
        "    \"某某候選人提出的土地公有政策\",\n",
        "    \"最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.\",\n",
        "    \"電子計算機是會計算題目的機器。\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "gQKcF1JFbToo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_seg = ws([\"高高興興打打球,丟丟看成績如何、開不開心\"])"
      ],
      "metadata": {
        "id": "GFJ6wM8idf_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_seg[0]"
      ],
      "metadata": {
        "id": "myknoTXzdmPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_pos = pos(sent_seg)\n",
        "sent_pos[0]"
      ],
      "metadata": {
        "id": "Iir58IfOmuJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}