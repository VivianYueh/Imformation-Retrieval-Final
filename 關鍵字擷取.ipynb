{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivianYueh/Imformation-Retrieval-Final/blob/main/%E9%97%9C%E9%8D%B5%E5%AD%97%E6%93%B7%E5%8F%96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>此處是使用處理中文句子斷詞的範例，包括CKIPtagger及Distiltag</h3>"
      ],
      "metadata": {
        "id": "7vVfZfoxqu6-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AuqXp259rhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1e5a0b-72f2-4174-fbc5-706fc79ff33a"
      },
      "source": [
        "!pip install -U DistilTag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting DistilTag\n",
            "  Using cached DistilTag-0.2.2-py3-none-any.whl.metadata (390 bytes)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (from DistilTag) (5.2.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from DistilTag) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=3.2 in /usr/local/lib/python3.11/dist-packages (from DistilTag) (4.51.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from DistilTag) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6->DistilTag)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->DistilTag) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->DistilTag) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.2->DistilTag) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown->DistilTag) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown->DistilTag) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6->DistilTag) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.2->DistilTag) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.2->DistilTag) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.2->DistilTag) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.2->DistilTag) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->DistilTag) (1.7.1)\n",
            "Using cached DistilTag-0.2.2-py3-none-any.whl (15 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, DistilTag\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed DistilTag-0.2.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvpYnakT9ro0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56fa7ff3-d1d4-4ed2-9ba2-2042e96522e0"
      },
      "source": [
        "import DistilTag\n",
        "DistilTag.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1AzUICPQ5MMt_IWg4JZ3mWM6vGbQkv01L\n",
            "From (redirected): https://drive.google.com/uc?id=1AzUICPQ5MMt_IWg4JZ3mWM6vGbQkv01L&confirm=t&uuid=844a4526-217c-47f9-b9f9-dd92981e6851\n",
            "To: /tmp/tmpsam65fw6distiltag/tagmodel.zip\n",
            "100%|██████████| 501M/501M [00:10<00:00, 46.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting up model...\n",
            "DistilTag model installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUIHi3IW9rr0"
      },
      "source": [
        "from DistilTag import DistilTag\n",
        "tagger = DistilTag()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 保留名詞、動詞"
      ],
      "metadata": {
        "id": "Wjnx1i8xPI4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_nouns_verbs(tagged):\n",
        "    \"\"\"\n",
        "    過濾出名詞(N*)與動詞(V*)的詞性標註\n",
        "    \"\"\"\n",
        "    return [(word, pos) for word, pos in tagged if pos.startswith('N') or pos.startswith('V')]\n"
      ],
      "metadata": {
        "id": "L-qLfT1UOBel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 擷取關鍵字"
      ],
      "metadata": {
        "id": "iHbE_QlCPpe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_words(words):\n",
        "    return list(dict.fromkeys(words))  # 去重同時保留原順序"
      ],
      "metadata": {
        "id": "JHF8Q8VrT7pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "base_url = 'https://www.niar.org.tw'\n",
        "news_base_url = 'https://www.niar.org.tw/xmdoc?xsmsid=0I148622737263495777'\n",
        "\n",
        "page = 1\n",
        "news_count = 0\n",
        "\n",
        "print(\"國家實驗研究院新聞列表：\\n\")\n",
        "\n",
        "while news_count<10:\n",
        "    url = f\"{news_base_url}&page={page}\"\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    rows = soup.find_all('tr')\n",
        "\n",
        "    found_news = False\n",
        "\n",
        "    for row in rows:\n",
        "        date_td = row.find('td', class_='date')\n",
        "        title_td = row.find('td', class_='title')\n",
        "\n",
        "        if date_td and title_td:\n",
        "            found_news = True\n",
        "            news_count += 1\n",
        "            date = date_td.text.strip()\n",
        "            a_tag = title_td.find('a')\n",
        "            title = a_tag.text.strip()\n",
        "            tags = tagger.tag(title)  # 這裡可能是 list[list[tuple]]\n",
        "            filtered = [filter_nouns_verbs(sent) for sent in tags]\n",
        "            words = [word for sent in filtered for word, pos in sent]\n",
        "            unique = unique_words(words)\n",
        "            link = base_url + a_tag['href']\n",
        "            print(f\"{news_count}. [{date}] {title}\\n   {tags}\\n   {unique}\\n   👉 {link}\\n\")\n",
        "\n",
        "    if not found_news:\n",
        "        break  # 沒有新內容就停止\n",
        "    page += 1"
      ],
      "metadata": {
        "id": "2_y__vUpr9TI",
        "outputId": "a3aad1e3-f498-4b26-90fb-7cfb60ed38a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "國家實驗研究院新聞列表：\n",
            "\n",
            "1. [2025-04-16] 科教館「科學家的秘密基地」 4/16更新展區重新亮相\n",
            "   [[('科教館', 'Nc'), ('「', 'PARENTHESISCATEGORY'), ('科學家', 'Na'), ('的', 'DE'), ('秘密', 'Na'), ('基地', 'Nc'), ('」', 'PARENTHESISCATEGORY'), ('４／１６', 'Nd'), ('更新', 'VC'), ('展區', 'Nc'), ('重新', 'D'), ('亮相', 'VA')]]\n",
            "   ['科教館', '科學家', '秘密', '基地', '４／１６', '更新', '展區', '亮相']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P106529706322578269\n",
            "\n",
            "2. [2025-04-14] 【國研院說明稿】關於小油坑失火事件  若鑑定結果為國研院設備造成 國研院絕不卸責\n",
            "   [[('【', 'PARENTHESISCATEGORY'), ('國研院', 'Nc'), ('說明稿', 'Na'), ('】', 'PARENTHESISCATEGORY'), ('關於', 'P'), ('小油坑', 'Nc'), ('失火', 'VH'), ('事件', 'Na'), ('若', 'Cbb'), ('鑑定', 'VC'), ('結果', 'Na'), ('為', 'P'), ('國研院', 'Nc'), ('設備', 'Na'), ('造成', 'VG'), ('國研院', 'Nc'), ('絕不', 'D'), ('卸責', 'VA')]]\n",
            "   ['國研院', '說明稿', '小油坑', '失火', '事件', '鑑定', '結果', '設備', '造成', '卸責']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P104744411632087760\n",
            "\n",
            "3. [2025-04-14] 【國研院說明稿】關於小油坑失火事件\n",
            "   [[('【', 'PARENTHESISCATEGORY'), ('國研院', 'Nc'), ('說明稿', 'Na'), ('】', 'PARENTHESISCATEGORY'), ('關於', 'P'), ('小油坑', 'Nc'), ('失火', 'VH'), ('事件', 'Na')]]\n",
            "   ['國研院', '說明稿', '小油坑', '失火', '事件']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P104624748119528484\n",
            "\n",
            "4. [2025-04-11] 臺法電子資訊與器官晶片深化交流 合作提升雙方科技研發量能\n",
            "   [[('臺', 'Nc'), ('法', 'Nc'), ('電子', 'Na'), ('資訊', 'Na'), ('與', 'Caa'), ('器官', 'Na'), ('晶片', 'Na'), ('深化', 'VHC'), ('交流', 'VH'), ('合作', 'VH'), ('提升', 'VC'), ('雙方', 'Nh'), ('科技', 'Na'), ('研發', 'Nv'), ('量能', 'Na')]]\n",
            "   ['臺', '法', '電子', '資訊', '器官', '晶片', '深化', '交流', '合作', '提升', '雙方', '科技', '研發', '量能']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P104350250093579190\n",
            "\n",
            "5. [2025-04-01] 國立陽明交通大學、振興醫院與國研院國儀中心 共同開發新型微米級光纖麥克風 為聽障者提供更無礙的聲音感受\n",
            "   [[('國立', 'A'), ('陽明', 'Nb'), ('交通', 'Na'), ('大學', 'Nc'), ('、', 'PAUSECATEGORY'), ('振興', 'VC'), ('醫院', 'Nc'), ('與', 'Caa'), ('國研院', 'Nc'), ('國儀', 'Na'), ('中心', 'Nc'), ('共同', 'A'), ('開發', 'VC'), ('新型', 'Na'), ('微米級', 'Na'), ('光纖', 'Na'), ('麥克風', 'Na'), ('為', 'P'), ('聽障者', 'Na'), ('提供', 'VD'), ('更', 'D'), ('無礙', 'VH'), ('的', 'DE'), ('聲音', 'Na'), ('感受', 'Na')]]\n",
            "   ['陽明', '交通', '大學', '振興', '醫院', '國研院', '國儀', '中心', '開發', '新型', '微米級', '光纖', '麥克風', '聽障者', '提供', '無礙', '聲音', '感受']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P091383779861602246\n",
            "\n",
            "6. [2025-04-01] TAIWAN AI RAP試營運啟動 徵求試營運用戶，免費算力助攻AI開發！\n",
            "   [[('ＴＡＩＷＡＮＡＩＲＡＰ', 'FW'), ('試', 'VF'), ('營運', 'Nv'), ('啟動', 'Na'), ('徵求', 'VC'), ('試', 'Nv'), ('營運', 'Nv'), ('用戶', 'Na'), ('，', 'COMMACATEGORY')], [('免費', 'VH'), ('算力', 'Na'), ('助攻', 'VA'), ('ＡＩ', 'DASHCATEGORY'), ('開發', 'VC'), ('！', 'EXCLANATIONCATEGORY')]]\n",
            "   ['試', '營運', '啟動', '徵求', '用戶', '免費', '算力', '助攻', '開發']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P091372518985390226\n",
            "\n",
            "7. [2025-03-28] 114年度「公民團體創新示範與沙盒試驗計畫」 徵件說明會\n",
            "   [[('１１４年度', 'Nd'), ('「', 'PARENTHESISCATEGORY'), ('公民', 'Na'), ('團體', 'Na'), ('創新', 'VC'), ('示範', 'Nv'), ('與', 'Caa'), ('沙盒', 'Na'), ('試驗', 'Na'), ('計畫', 'Na'), ('」', 'PARENTHESISCATEGORY'), ('徵件', 'Na'), ('說明', 'Na'), ('會', 'D')]]\n",
            "   ['１１４年度', '公民', '團體', '創新', '示範', '沙盒', '試驗', '計畫', '徵件', '說明']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P087618875405814531\n",
            "\n",
            "8. [2025-03-25] 國家實驗研究院英文名稱更名揭牌\n",
            "   [[('國家', 'Na'), ('實驗', 'Na'), ('研究院', 'Nc'), ('英文', 'Na'), ('名稱', 'Na'), ('更名', 'VA'), ('揭牌', 'VA')]]\n",
            "   ['國家', '實驗', '研究院', '英文', '名稱', '更名', '揭牌']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P084523018471266747\n",
            "\n",
            "9. [2025-03-24] 國網中心晶創主機Nano 5徵案啟動 南台灣半導體業搶搭高效能運算列車\n",
            "   [[('國網', 'Na'), ('中心', 'Nc'), ('晶創', 'Nb'), ('主機', 'Na'), ('Ｎａｎｏ５', 'FW'), ('徵案', 'Na'), ('啟動', 'VC'), ('南', 'Ncd'), ('台灣', 'Nc'), ('半導體業', 'Na'), ('搶搭', 'VC'), ('高效能', 'Na'), ('運算', 'VC'), ('列車', 'Na')]]\n",
            "   ['國網', '中心', '晶創', '主機', '徵案', '啟動', '南', '台灣', '半導體業', '搶搭', '高效能', '運算', '列車']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P083555590774151742\n",
            "\n",
            "10. [2025-03-19] 國研院x國資圖 科學家的秘密基地@臺中 「地震工程大解密－讓建築更安全的秘密」地震科普展 房屋耐震，大家一起來！\n",
            "   [[('國研院', 'Nc'), ('ｘ', 'DASHCATEGORY'), ('國資圖', 'Na'), ('科學家', 'Na'), ('的', 'DE'), ('秘密', 'Na'), ('基地', 'Nc'), ('＠', 'FW'), ('臺中', 'Nc'), ('「', 'PARENTHESISCATEGORY'), ('地震', 'Nv'), ('工程', 'Na'), ('大解密', 'VA'), ('－', 'FW'), ('讓', 'VL'), ('建築', 'Na'), ('更', 'D'), ('安全', 'VH'), ('的', 'DE'), ('秘密', 'Na'), ('」', 'PARENTHESISCATEGORY'), ('地震科', 'Nc'), ('普展', 'VC'), ('房屋', 'Na'), ('耐震', 'VH'), ('，', 'COMMACATEGORY')], [('大家', 'Nh'), ('一起', 'D'), ('來', 'VA'), ('！', 'EXCLANATIONCATEGORY')]]\n",
            "   ['國研院', '國資圖', '科學家', '秘密', '基地', '臺中', '地震', '工程', '大解密', '讓', '建築', '安全', '地震科', '普展', '房屋', '耐震', '大家', '來']\n",
            "   👉 https://www.niar.org.tw/xmdoc/cont?xsmsid=0I148622737263495777&sid=0P078409728113064068\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 內文擷取"
      ],
      "metadata": {
        "id": "5RQh2-aiU6DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import textwrap\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "dict_list = []\n",
        "latest_saved_date = None\n",
        "\n",
        "json_file = \"content_data.json\"\n",
        "dict_list = []\n",
        "latest_saved_date = None\n",
        "\n",
        "if os.path.exists(json_file):\n",
        "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        try:\n",
        "            dict_list = json.load(f)\n",
        "            if dict_list:\n",
        "                latest_saved_date = datetime.strptime(dict_list[0][\"date\"], \"%Y-%m-%d\")\n",
        "                print(f\"  已找到 {json_file}，最新日期為：{latest_saved_date.date()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  讀取 JSON 發生錯誤：{e}，將從頭開始抓取\")\n",
        "else:\n",
        "    print(f\"  未發現 {json_file}，將從頭開始抓資料（最多 {200} 筆）\")\n",
        "\n",
        "\n",
        "base_url = 'https://www.niar.org.tw'\n",
        "news_base_url = 'https://www.niar.org.tw/xmdoc?xsmsid=0I148622737263495777'\n",
        "\n",
        "page = 1\n",
        "news_count = 0\n",
        "new_items = []\n",
        "stop=False\n",
        "\n",
        "#print(\"國家實驗研究院新聞列表：\\n\")\n",
        "\n",
        "while not stop:\n",
        "    url = f\"{news_base_url}&page={page}\"\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    rows = soup.find_all('tr')\n",
        "\n",
        "    found_news = False\n",
        "\n",
        "    for row in rows:\n",
        "        date_td = row.find('td', class_='date')\n",
        "        title_td = row.find('td', class_='title')\n",
        "\n",
        "        if date_td and title_td:\n",
        "            found_news = True\n",
        "            news_count += 1\n",
        "            date = date_td.text.strip()\n",
        "\n",
        "            try:\n",
        "              date_obj = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "            except ValueError:\n",
        "              continue  # 跳過無法解析的日期\n",
        "\n",
        "            # 判斷是否是舊資料（小於或等於）\n",
        "            if latest_saved_date and date_obj <= latest_saved_date:\n",
        "              stop = True\n",
        "              break\n",
        "\n",
        "            a_tag = title_td.find('a')\n",
        "            title = a_tag.text.strip()\n",
        "            link = base_url + a_tag['href']\n",
        "            article_response = requests.get(link)\n",
        "            article_response.encoding = 'utf-8'\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "            # 抓取文章內容\n",
        "            article_response = requests.get(link)\n",
        "            article_response.encoding = 'utf-8'\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "            content = article_soup.find_all('p', class_='MsoNormal')\n",
        "            content_text = \"\\n\".join(p.get_text(strip=True)  for p in content  if p.get_text(strip=True))\n",
        "            #print(f\"{news_count}. {title}\\n   {content_text}\\n\")\n",
        "            #print(f\"{news_count}. [{date}] {title}\\n   {tags}\\n   {unique}\\n   👉 {link}\\n\")\n",
        "\n",
        "            content_dict={\"title\":title,\"date\":date,\"content\":content_text,\"link\":link}\n",
        "            new_items.append(content_dict)\n",
        "\n",
        "            if not latest_saved_date and len(new_items) >= 200:\n",
        "              stop = True\n",
        "              break\n",
        "\n",
        "    print(f\"\\n  共新增 {len(new_items)} 筆新聞資料\")\n",
        "\n",
        "    if not found_news:\n",
        "        break  # 沒有新內容就停止\n",
        "    page += 1\n",
        "\n",
        "\n",
        "if new_items:\n",
        "  dict_list = new_items + dict_list\n",
        "  print(f\"\\n  共新增 {len(new_items)} 筆新聞資料\")\n",
        "  with open(\"content_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(dict_list, f, ensure_ascii=False, indent=2)\n",
        "files.download(\"content_data.json\")"
      ],
      "metadata": {
        "id": "2a8iguHYU9N8",
        "outputId": "00f79fb2-60a0-4c58-ff0c-abf17be4ad8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  未發現 content_data.json，將從頭開始抓資料（最多 200 筆）\n",
            "\n",
            "  共新增 10 筆新聞資料\n",
            "\n",
            "  共新增 20 筆新聞資料\n",
            "\n",
            "  共新增 30 筆新聞資料\n",
            "\n",
            "  共新增 40 筆新聞資料\n",
            "\n",
            "  共新增 50 筆新聞資料\n",
            "\n",
            "  共新增 60 筆新聞資料\n",
            "\n",
            "  共新增 70 筆新聞資料\n",
            "\n",
            "  共新增 80 筆新聞資料\n",
            "\n",
            "  共新增 90 筆新聞資料\n",
            "\n",
            "  共新增 100 筆新聞資料\n",
            "\n",
            "  共新增 110 筆新聞資料\n",
            "\n",
            "  共新增 120 筆新聞資料\n",
            "\n",
            "  共新增 130 筆新聞資料\n",
            "\n",
            "  共新增 140 筆新聞資料\n",
            "\n",
            "  共新增 150 筆新聞資料\n",
            "\n",
            "  共新增 160 筆新聞資料\n",
            "\n",
            "  共新增 170 筆新聞資料\n",
            "\n",
            "  共新增 180 筆新聞資料\n",
            "\n",
            "  共新增 190 筆新聞資料\n",
            "\n",
            "  共新增 200 筆新聞資料\n",
            "\n",
            "  共新增 200 筆新聞資料\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d405a57a-eb11-45ad-bb52-1369e779c592\", \"content_data.json\", 704528)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4/17"
      ],
      "metadata": {
        "id": "7jQnPFKzTSN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import textwrap\n",
        "from collections import Counter\n",
        "\n",
        "base_url = 'https://www.niar.org.tw'\n",
        "news_base_url = 'https://www.niar.org.tw/xmdoc?xsmsid=0I148622737263495777'\n",
        "\n",
        "page = 1\n",
        "news_count = 0\n",
        "\n",
        "special_terms = [\n",
        "    \"國家生物模式中心\",\n",
        "    \"國家地震工程研究中心\",\n",
        "    \"國家高速網路與計算中心\",\n",
        "    \"台灣半導體研究中心\",\n",
        "    \"國家儀器科技研究中心\",\n",
        "    \"科技政策研究與資訊中心\",\n",
        "    \"台灣海洋科技研究中心\"\n",
        "]\n",
        "\n",
        "print(\"國家實驗研究院新聞列表：\\n\")\n",
        "\n",
        "while news_count<10:\n",
        "    url = f\"{news_base_url}&page={page}\"\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    rows = soup.find_all('tr')\n",
        "\n",
        "    found_news = False\n",
        "\n",
        "    for row in rows:\n",
        "        date_td = row.find('td', class_='date')\n",
        "        title_td = row.find('td', class_='title')\n",
        "\n",
        "        if date_td and title_td:\n",
        "            found_news = True\n",
        "            news_count += 1\n",
        "            #date = date_td.text.strip()\n",
        "            a_tag = title_td.find('a')\n",
        "            title = a_tag.text.strip()\n",
        "            '''tags = tagger.tag(title)  # 這裡可能是 list[list[tuple]]\n",
        "            filtered = [filter_nouns_verbs(sent) for sent in tags]\n",
        "            words = [word for sent in filtered for word, pos in sent]\n",
        "            unique = unique_words(words)'''\n",
        "            link = base_url + a_tag['href']\n",
        "            article_response = requests.get(link)\n",
        "            article_response.encoding = 'utf-8'\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "            # 抓取文章內容\n",
        "            article_response = requests.get(link)\n",
        "            article_response.encoding = 'utf-8'\n",
        "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "            content = article_soup.find_all('p', class_='MsoNormal')\n",
        "            content_text = \"\\n\".join(p.get_text(strip=True)  for p in content  if p.get_text(strip=True))\n",
        "\n",
        "            found_special_terms = [term for term in special_terms if term in content_text]\n",
        "\n",
        "            tags = tagger.tag(content_text)\n",
        "            filtered = [filter_nouns_verbs(sent) for sent in tags]\n",
        "            words = [word for sent in filtered for word, pos in sent]\n",
        "\n",
        "            for term in found_special_terms:\n",
        "              for subword in term:\n",
        "                  words = [word for word in words if word != subword]\n",
        "\n",
        "            words += found_special_terms\n",
        "\n",
        "            counter = Counter(words)\n",
        "            top_10 = counter.most_common(10)\n",
        "            print(f\"{news_count}. {title}\\n   {top_10}\\n   {found_special_terms}\\n\")\n",
        "\n",
        "            #print(f\"{news_count}. {title}\\n   {content_text}\\n\")\n",
        "            #print(f\"{news_count}. [{date}] {title}\\n   {tags}\\n   {unique}\\n   👉 {link}\\n\")\n",
        "\n",
        "    if not found_news:\n",
        "        break  # 沒有新內容就停止\n",
        "    page += 1\n"
      ],
      "metadata": {
        "id": "fDthf2FObAoo",
        "outputId": "f4f18b48-f709-4cee-97ae-fd614b139c78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "國家實驗研究院新聞列表：\n",
            "\n",
            "1. 科教館「科學家的秘密基地」 4/16更新展區重新亮相\n",
            "   [('中心', 17), ('海洋', 13), ('基地', 12), ('研究', 8), ('國家', 7), ('動物', 7), ('實驗', 6), ('參觀', 6), ('基因', 6), ('者', 6)]\n",
            "   ['國家生物模式中心', '國家地震工程研究中心', '台灣海洋科技研究中心']\n",
            "\n",
            "2. 【國研院說明稿】關於小油坑失火事件  若鑑定結果為國研院設備造成 國研院絕不卸責\n",
            "   [('國研院', 8), ('中心', 5), ('陽明山', 3), ('國家', 3), ('表示', 3), ('國網', 3), ('設備', 3), ('調查', 3), ('空氣', 3), ('資料', 3)]\n",
            "   ['國家高速網路與計算中心']\n",
            "\n",
            "3. 【國研院說明稿】關於小油坑失火事件\n",
            "   [('空氣', 6), ('品質', 6), ('感測器', 5), ('國家', 4), ('地區', 3), ('中心', 2), ('小油坑', 2), ('陽明山', 2), ('公園', 2), ('本', 2)]\n",
            "   ['國家高速網路與計算中心']\n",
            "\n",
            "4. 臺法電子資訊與器官晶片深化交流 合作提升雙方科技研發量能\n",
            "   [('研究', 12), ('法國', 11), ('技術', 10), ('器官', 9), ('國家', 8), ('國研院', 7), ('合作', 7), ('晶片', 7), ('研究院', 5), ('交流', 5)]\n",
            "   ['國家生物模式中心', '國家儀器科技研究中心']\n",
            "\n",
            "5. 國立陽明交通大學、振興醫院與國研院國儀中心 共同開發新型微米級光纖麥克風 為聽障者提供更無礙的聲音感受\n",
            "   [('麥克風', 13), ('光纖', 10), ('聲音', 7), ('中心', 7), ('國儀', 6), ('應用', 5), ('薄膜', 5), ('此', 4), ('新型', 4), ('微米級', 4)]\n",
            "   ['國家儀器科技研究中心']\n",
            "\n",
            "6. TAIWAN AI RAP試營運啟動 徵求試營運用戶，免費算力助攻AI開發！\n",
            "   [('開發', 19), ('應用', 15), ('中心', 7), ('服務', 7), ('國網', 6), ('者', 6), ('工具', 5), ('技術', 5), ('申請', 5), ('模型', 5)]\n",
            "   ['國家高速網路與計算中心']\n",
            "\n",
            "7. 114年度「公民團體創新示範與沙盒試驗計畫」 徵件說明會\n",
            "   [('淨零', 13), ('計畫', 12), ('社區', 11), ('沙盒', 10), ('推動', 8), ('創新', 7), ('小組', 5), ('實驗', 4), ('執行', 4), ('公民', 4)]\n",
            "   []\n",
            "\n",
            "8. 國家實驗研究院英文名稱更名揭牌\n",
            "   [('國家', 18), ('中心', 18), ('國研院', 15), ('研究', 12), ('科技', 9), ('英文', 6), ('實驗', 5), ('名稱', 5), ('簡稱', 5), ('台灣', 5)]\n",
            "   ['國家生物模式中心', '國家地震工程研究中心', '國家高速網路與計算中心', '台灣半導體研究中心', '國家儀器科技研究中心', '科技政策研究與資訊中心', '台灣海洋科技研究中心']\n",
            "\n",
            "9. 國網中心晶創主機Nano 5徵案啟動 南台灣半導體業搶搭高效能運算列車\n",
            "   [('半導體', 14), ('產業', 12), ('技術', 10), ('創新', 9), ('運算', 9), ('企業', 8), ('中心', 8), ('應用', 7), ('國網', 7), ('高效能', 7)]\n",
            "   ['國家高速網路與計算中心']\n",
            "\n",
            "10. 國研院x國資圖 科學家的秘密基地@臺中 「地震工程大解密－讓建築更安全的秘密」地震科普展 房屋耐震，大家一起來！\n",
            "   [('地震', 20), ('工程', 10), ('建築', 10), ('耐震', 8), ('技術', 8), ('臺灣', 7), ('安全', 7), ('民眾', 6), ('讓', 5), ('房屋', 5)]\n",
            "   ['國家地震工程研究中心']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YaGGiYvYcTHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ckiptagger"
      ],
      "metadata": {
        "id": "25VMyybXPVGN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikdo30qW9rvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514f88fa-bc81-4b3f-9007-519930aad1ea"
      },
      "source": [
        "!pip install -U ckip-transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ckip-transformers\n",
            "  Downloading ckip_transformers-0.3.4-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from ckip-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from ckip-transformers) (4.67.1)\n",
            "Requirement already satisfied: transformers>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from ckip-transformers) (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.5.0->ckip-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->ckip-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->ckip-transformers) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.5.0->ckip-transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.5.0->ckip-transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.5.0->ckip-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.5.0->ckip-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.5.0->ckip-transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=3.5.0->ckip-transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.5.0->ckip-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.5.0->ckip-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->ckip-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (2025.1.31)\n",
            "Downloading ckip_transformers-0.3.4-py3-none-any.whl (26 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ckip-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed ckip-transformers-0.3.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker"
      ],
      "metadata": {
        "id": "KHS8fZAYVKh5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize drivers\n",
        "ws_driver  = CkipWordSegmenter(model=\"bert-base\")\n",
        "pos_driver = CkipPosTagger(model=\"bert-base\")\n",
        "ner_driver = CkipNerChunker(model=\"bert-base\")"
      ],
      "metadata": {
        "id": "S3JFsiu4VbNY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "existing_content=[]\n",
        "if os.path.exists(\"content_data.json\"):\n",
        "    with open(\"content_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        try:\n",
        "            dict_list = json.load(f)\n",
        "            existing_content = {dict_list[i][\"content\"] for i in range(0,5)}\n",
        "        except Exception as e:\n",
        "            print(\"讀取舊資料失敗\")"
      ],
      "metadata": {
        "id": "8UafJWTaKkA4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ws  = ws_driver(existing_content)\n",
        "pos = pos_driver(ws)\n",
        "ner = ner_driver(existing_content)"
      ],
      "metadata": {
        "id": "9nU0OCAiPqTi",
        "outputId": "8dbf926f-f1b5-497a-840a-d3aa82c5427c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenization: 100%|██████████| 5/5 [00:00<00:00, 286.68it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:17<00:00, 17.45s/it]\n",
            "Tokenization: 100%|██████████| 5/5 [00:00<00:00, 647.75it/s]\n",
            "Inference: 100%|██████████| 1/1 [01:24<00:00, 84.72s/it]\n",
            "Tokenization: 100%|██████████| 5/5 [00:00<00:00, 623.26it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:18<00:00, 18.25s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pack word segmentation and part-of-speech results\n",
        "def pack_ws_pos_sentece(sentence_ws, sentence_pos):\n",
        "   assert len(sentence_ws) == len(sentence_pos)\n",
        "   res = []\n",
        "   for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
        "      res.append(f\"{word_ws}({word_pos})\")\n",
        "   return \"\\u3000\".join(res)\n",
        "\n",
        "# Show results\n",
        "for sentence, sentence_ws, sentence_pos, sentence_ner in zip(existing_content, ws, pos, ner):\n",
        "   print(sentence)\n",
        "   print(pack_ws_pos_sentece(sentence_ws, sentence_pos))\n",
        "   for entity in sentence_ner:\n",
        "      print(entity)\n",
        "   print()"
      ],
      "metadata": {
        "id": "Kn2lmRp0vrDN",
        "outputId": "9e0a6962-9ea9-4868-97a5-ee8c77ea2590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "國科會轄下之國家實驗研究院及國家太空中心，與國立臺灣科學教育館合作，辦理「科學家的秘密基地」長期展，自2023年3月開展以來，備受各界好評，已吸引超過13萬人入場參觀。今（4/16）日於更新部分展品後重新開展，希望能幫助參觀民眾透過互動遊戲、親眼觀察與模型展示，習得有趣的科學知識，同時深入認識科研工作。「科學家的秘密基地」位於科教館8樓東南側扇形展場，分為「實驗基地」、「探測基地」和「智慧基地」三區，每一區均包含兩大展示主題。此次更新的是國家生物模式中心（原國家實驗動物中心）、國家地震工程研究中心（國震中心）及台灣海洋科技研究中心（海洋中心）的展品。一進門就看到實驗基地－生物模式中心展出的「貓咪毛色變化的秘密」，這裡展示多基因協同作用的觀念。生物模式中心近年以革新的基因編輯工具「CRISPR/Cas」基因剪刀產製「擬人鼠」動物模式，推進我國精準醫療研究與臨床應用，而科學家進行基因編輯時，須嚴加考慮生物體複雜的基因層次。現場除了深入淺出的科學觀念，也設計了一個基因密碼轉盤遊戲，參觀者可以透過簡單的科普工具，輕鬆學習生物醫學知識。實驗基地－國震中心透過互動展示，介紹房屋的共振原理，包括不同樓高建築物的振動特性，以及地震波在不同地質條件下傳遞所產生的共振效應。現場提供以不同長度的吸管模擬各種樓高的共振教具，讓參觀者能親身體驗結構與地震波共振的現象。同時展示「滾動式隔震平台」，結合電動振動台模擬真實地震波，可清楚觀察放置於隔震裝置上的設備，其振動幅度大幅減少，讓參觀者直觀感受隔震技術的保護效果。接著來到探測基地－海洋中心，這裡展出於臺灣西南海域及南海採集到的海洋浮游動物標本、寫真、採集工具，以及使用海洋中心自製底碇平台記錄之水下300米的海底世界。海洋浮游動物在海洋生態系中扮演著重要而多樣的角色，牠們的種類及數量非常容易受到海洋環境的影響，當環境變動時（如溫度、鹽度、酸鹼度），浮游動物的組成也會隨之改變，間接對魚類、鯨類等各種海洋動物產生連鎖反應。因此浮游動物是一種很好的環境指標生物，可以用來監測海洋環境變遷、預測海洋生態系的改變。展場設置顯微觀察區，讓參觀者一窺這群嬌小但至關重要的小小兵身影，了解牠們是維護海洋生態平衡的關鍵多數。除了更新的三件展品外，探測基地－太空中心展出我國第一顆自製氣象衛星獵風者衛星（Triton）的1：1模型及其元件介紹。獵風者衛星搭載國家太空中心自行研發的「全球導航衛星系統反射訊號接收儀」（GNSS-R），進行海洋風場、海面高度、海氣交互作用、地表含水量、颱風強度預測等研究。智慧基地－科政中心設計出一套多媒體互動遊戲，有三題選擇題，題目出自「PRIDE政策研究指標資料庫」，觀眾只要揮動手臂就可以在螢幕上選擇答案，在答題的過程中對我們的世界現況建立基本認識。智慧基地－國網中心展出「搜救犬日常訓練成果三維虛擬導覽系統」，展現搜救犬中心的全方位實境景觀，可作為訓練領犬員及搜救犬之參考，也讓民眾認識犬隻訓練過程和了解搜救任務。國研院近年來積極把國家級實驗研究單位開發的尖端科技，轉化為讓中小學生都能有基本認識的科普展覽與活動，希望能幫助中小學生建立對科學的興趣，並培養科學素養。「科學家的秘密基地」會定期進行展品更新，持續提供創新的國家研究成果，歡迎民眾來科教館8樓探索尖端科技研究的有趣之處。\n",
            "國科會(Nc)　轄下(Nc)　之(DE)　國家(Na)　實驗(Na)　研究院(Nc)　及(Caa)　國家(Na)　太空(Na)　中心(Nc)　，(COMMACATEGORY)　與(P)　國立(A)　臺灣(Nc)　科學(Na)　教育館(Nc)　合作(VH)　，(COMMACATEGORY)　辦理(VC)　「(PARENTHESISCATEGORY)　科學家(Na)　的(DE)　秘密(VH)　基地(Nc)　」(PARENTHESISCATEGORY)　長期展(Na)　，(COMMACATEGORY)　自(P)　2023年(Neu)　3月(Nd)　開展(VC)　以來(Ng)　，(COMMACATEGORY)　備受(P)　各界(Na)　好評(Na)　，(COMMACATEGORY)　已(D)　吸引(VJ)　超過(VJ)　13萬(Neu)　人(Na)　入場(VA)　參觀(VC)　。(PERIODCATEGORY)　今(Nd)　（4/16(PARENTHESISCATEGORY)　）(PARENTHESISCATEGORY)　日(Nd)　於(P)　更新(VC)　部分(Neqa)　展品(Na)　後(Ng)　重新(D)　開展(VC)　，(COMMACATEGORY)　希望(VK)　能(D)　幫助(VC)　參觀(Nv)　民眾(Na)　透過(P)　互動(Nv)　遊戲(Na)　、(PAUSECATEGORY)　親眼(D)　觀察(VE)　與(Caa)　模型(Na)　展示(Nv)　，(COMMACATEGORY)　習得(VC)　有趣(VH)　的(DE)　科學(Na)　知識(Na)　，(COMMACATEGORY)　同時(Nd)　深入(VH)　認識(VJ)　科研(Na)　工作(Na)　。(PERIODCATEGORY)　「(PARENTHESISCATEGORY)　科學家(Na)　的(DE)　秘密(VH)　基地(Nc)　」(PARENTHESISCATEGORY)　位於(VCL)　科教館(Nc)　8樓(Nc)　東南側(Ncd)　扇形(Na)　展場(Nc)　，(COMMACATEGORY)　分為(VG)　「(PARENTHESISCATEGORY)　實驗(Na)　基地(Nc)　」(PARENTHESISCATEGORY)　、(PAUSECATEGORY)　「(PARENTHESISCATEGORY)　探測(Nv)　基地(Nc)　」(PARENTHESISCATEGORY)　和(Caa)　「(PARENTHESISCATEGORY)　智慧(Na)　基地(Nc)　」(PARENTHESISCATEGORY)　三(Neu)　區(Nc)　，(COMMACATEGORY)　每(Nes)　一(Neu)　區(Nc)　均(D)　包含(VJ)　兩(Neu)　大(VH)　展示(Nv)　主題(Na)　。(PERIODCATEGORY)　此(Nep)　次(Nf)　更新(Dfa)　的(DE)　是(SHI)　國家(Na)　生物(Na)　模式(Na)　中心(Nc)　（(PARENTHESISCATEGORY)　原(A)　國家(Na)　實驗(Na)　動物(Na)　中心(Nc)　）(PARENTHESISCATEGORY)　、(PAUSECATEGORY)　國家(Na)　地震(VA)　工程(Na)　研究(VE)　中心(Nc)　（(PARENTHESISCATEGORY)　國震(Na)　中心(Nc)　）(PARENTHESISCATEGORY)　及(Caa)　台灣(Nc)　海洋(Na)　科技(Na)　研究(VE)　中心(Nc)　（(PARENTHESISCATEGORY)　海洋(Na)　中心(Nc)　）(PARENTHESISCATEGORY)　的(DE)　展品(Na)　。(PERIODCATEGORY)　一(D)　進(VCL)　門(Na)　就(D)　看到(VE)　實驗(Na)　基地(Nc)　－(DASHCATEGORY)　生物(Na)　模式(Na)　中心(Nc)　展出(VC)　的(DE)　「(PARENTHESISCATEGORY)　貓咪(Na)　毛色(Na)　變化(Na)　的(DE)　秘密(Na)　」(PARENTHESISCATEGORY)　，(COMMACATEGORY)　這裡(Ncd)　展示(VC)　多(Na)　基因(Na)　協同(A)　作用(Na)　的(DE)　觀念(Na)　。(PERIODCATEGORY)　生物(Na)　模式(Na)　中心(Nc)　近年(Nd)　以(P)　革新(VC)　的(DE)　基因(Na)　編輯(VC)　工具(Na)　「(PARENTHESISCATEGORY)　CRISPR/Cas(FW)　」(PARENTHESISCATEGORY)　基因(Na)　剪刀(Na)　產製(VC)　「(PARENTHESISCATEGORY)　擬人鼠(Na)　」(PARENTHESISCATEGORY)　動物(Na)　模式(Na)　，(COMMACATEGORY)　推進(VAC)　我國(Nc)　精準(VH)　醫療(VC)　研究(VE)　與(Caa)　臨床(A)　應用(Na)　，(COMMACATEGORY)　而(Cbb)　科學家(Na)　進行(VC)　基因(Na)　編輯(VC)　時(Ng)　，(COMMACATEGORY)　須(D)　嚴加(D)　考慮(VE)　生物體(Na)　複雜(VH)　的(DE)　基因(Na)　層次(Na)　。(PERIODCATEGORY)　現場(Nc)　除了(P)　深入淺出(VH)　的(DE)　科學(Na)　觀念(Na)　，(COMMACATEGORY)　也(D)　設計(VC)　了(Di)　一(Neu)　個(Nf)　基因(Na)　密碼(Na)　轉盤(Na)　遊戲(Na)　，(COMMACATEGORY)　參觀(VC)　者(Na)　可以(D)　透過(P)　簡單(VH)　的(DE)　科普(Na)　工具(Na)　，(COMMACATEGORY)　輕鬆(VH)　學習(VC)　生物(Na)　醫學(Na)　知識(Na)　。(PERIODCATEGORY)　實驗(VE)　基地(Nc)　－(DASHCATEGORY)　國震(Na)　中心(Nc)　透過(P)　互動(VA)　展示(VC)　，(COMMACATEGORY)　介紹(VE)　房屋(Na)　的(DE)　共振(VH)　原理(Na)　，(COMMACATEGORY)　包括(VK)　不同(VH)　樓高(Nc)　建築物(Na)　的(DE)　振動(VAC)　特性(Na)　，(COMMACATEGORY)　以及(Caa)　地震波(Na)　在(P)　不同(VH)　地質(Na)　條件(Na)　下(Ng)　傳遞(VD)　所(D)　產生(VHC)　的(DE)　共振(VH)　效應(Na)　。(PERIODCATEGORY)　現場(Nc)　提供(VD)　以(P)　不同(VH)　長度(Na)　的(DE)　吸管(Na)　模擬(VC)　各(Nes)　種(Nf)　樓(Nc)　高(VH)　的(DE)　共振(VH)　教具(Na)　，(COMMACATEGORY)　讓(VL)　參觀(VC)　者(Na)　能(D)　親身(D)　體驗(VC)　結構(Na)　與(Caa)　地震波(Na)　共振(VH)　的(DE)　現象(Na)　。(PERIODCATEGORY)　同時(Nd)　展示(VC)　「(PARENTHESISCATEGORY)　滾動式(Na)　隔震(VH)　平台(Na)　」(PARENTHESISCATEGORY)　，(COMMACATEGORY)　結合(VHC)　電動(A)　振動台(Na)　模擬(VC)　真實(VH)　地震波(Na)　，(COMMACATEGORY)　可(D)　清楚(VH)　觀察(VE)　放置(VC)　於(P)　隔震(VH)　裝置(Na)　上(Ncd)　的(DE)　設備(Na)　，(COMMACATEGORY)　其(Nep)　振動(VAC)　幅度(Na)　大幅(D)　減少(VHC)　，(COMMACATEGORY)　讓(VL)　參觀(VC)　者(Na)　直觀(A)　感受(VK)　隔震(VH)　技術(Na)　的(DE)　保護(VC)　效果(Na)　。(PERIODCATEGORY)　接著(D)　來到(VCL)　探測(VC)　基地(Nc)　－(DASHCATEGORY)　海洋(Na)　中心(Nc)　，(COMMACATEGORY)　這裡(Ncd)　展出(VC)　於(P)　臺灣(Nc)　西南(Ncd)　海域(Na)　及(Caa)　南海(Nc)　採集到(VC)　的(DE)　海洋(Na)　浮游(Nv)　動物(Na)　標本(Na)　、(PAUSECATEGORY)　寫真(Na)　、(PAUSECATEGORY)　採集(Nv)　工具(Na)　，(COMMACATEGORY)　以及(Caa)　使用(VC)　海洋(Na)　中心(Nc)　自製(VC)　底碇(Na)　平台(Na)　記錄(Na)　之(DE)　水(Na)　下(Ncd)　300(Neu)　米(Nf)　的(DE)　海(Na)　底(Ncd)　世界(Nc)　。(PERIODCATEGORY)　海洋(Na)　浮游(VA)　動物(Na)　在(P)　海洋(Na)　生態系(Na)　中(Ng)　扮演(VC)　著(Di)　重要(VH)　而(Caa)　多樣(VH)　的(DE)　角色(Na)　，(COMMACATEGORY)　牠們(Nh)　的(DE)　種類(Na)　及(Caa)　數量(Na)　非常(Dfa)　容易(VH)　受到(VJ)　海洋(Na)　環境(Na)　的(DE)　影響(Na)　，(COMMACATEGORY)　當(P)　環境(Na)　變動(VHC)　時(Ng)　（(PARENTHESISCATEGORY)　如(P)　溫度(Na)　、(PAUSECATEGORY)　鹽度(Na)　、(PAUSECATEGORY)　酸鹼度(Na)　）(PARENTHESISCATEGORY)　，(COMMACATEGORY)　浮游(Nv)　動物(Na)　的(DE)　組成(Nv)　也(D)　會(D)　隨之(P)　改變(VC)　，(COMMACATEGORY)　間接(D)　對(P)　魚類(Na)　、(PAUSECATEGORY)　鯨類(Na)　等(Cab)　各(Nes)　種(Nf)　海洋(Na)　動物(Na)　產生(VHC)　連鎖(A)　反應(Na)　。(PERIODCATEGORY)　因此(Cbb)　浮游(Nv)　動物(Na)　是(SHI)　一(Neu)　種(Nf)　很(Dfa)　好(VH)　的(DE)　環境(Na)　指標(Na)　生物(Na)　，(COMMACATEGORY)　可以(D)　用來(VL)　監測(VC)　海洋(Na)　環境(Na)　變遷(VH)　、(PAUSECATEGORY)　預測(VE)　海洋(Na)　生態系(Na)　的(DE)　改變(VC)　。(PERIODCATEGORY)　展場(Nc)　設置(VC)　顯微(Na)　觀察區(Nc)　，(COMMACATEGORY)　讓(VL)　參觀(VC)　者(Na)　一(D)　窺(VC)　這(Nep)　群(Nf)　嬌小(VH)　但(Cbb)　至關(Dfa)　重要(VH)　的(DE)　小小兵(Na)　身影(Na)　，(COMMACATEGORY)　了解(VK)　牠們(Nh)　是(SHI)　維護(VC)　海洋(Na)　生態(Na)　平衡(VHC)　的(DE)　關鍵(Na)　多數(Neqa)　。(PERIODCATEGORY)　除了(P)　更新(VC)　的(DE)　三(Neu)　件(Nf)　展品(Na)　外(Ng)　，(COMMACATEGORY)　探測(VC)　基地(Nc)　－(FW)　太空(Na)　中心(Nc)　展出(VC)　我國(Nc)　第一(Neu)　顆(Nf)　自製(VC)　氣象(Na)　衛星(Na)　獵風者(VA)　衛星(Na)　（(PARENTHESISCATEGORY)　Triton(FW)　）(PARENTHESISCATEGORY)　的(DE)　1：1(Neu)　模型(Na)　及(Caa)　其(Nep)　元件(Na)　介紹(VE)　。(PERIODCATEGORY)　獵風者(Nb)　衛星(Na)　搭載(VC)　國家(Na)　太空(Na)　中心(Nc)　自行(D)　研發(VC)　的(DE)　「(PARENTHESISCATEGORY)　全球(Nc)　導航(VB)　衛星(Na)　系統(Na)　反射(VJ)　訊號(Na)　接收(Na)　儀(Na)　」(PARENTHESISCATEGORY)　（(PARENTHESISCATEGORY)　GNSS-R(FW)　）(PARENTHESISCATEGORY)　，(COMMACATEGORY)　進行(VC)　海洋(Na)　風場(Na)　、(PAUSECATEGORY)　海面(Nc)　高度(Na)　、(PAUSECATEGORY)　海氣(Na)　交互作用(Na)　、(PAUSECATEGORY)　地表(Nc)　含水量(Na)　、(PAUSECATEGORY)　颱風(Na)　強度(Na)　預測(VE)　等(Cab)　研究(VE)　。(PERIODCATEGORY)　智慧(Na)　基地(Nc)　－(DASHCATEGORY)　科政(Na)　中心(Nc)　設計出(VC)　一(Neu)　套(Nf)　多媒體(Na)　互動(VA)　遊戲(Na)　，(COMMACATEGORY)　有(V_2)　三(Neu)　題(Nf)　選擇題(Na)　，(COMMACATEGORY)　題目(Na)　出自(VJ)　「(PARENTHESISCATEGORY)　PRIDE(FW)　政策(Na)　研究(VE)　指標(Na)　資料庫(Na)　」(PARENTHESISCATEGORY)　，(COMMACATEGORY)　觀眾(Na)　只要(Cbb)　揮動(VC)　手臂(Na)　就(D)　可以(D)　在(P)　螢幕(Na)　上(Ncd)　選擇(VC)　答案(Na)　，(COMMACATEGORY)　在(P)　答(VE)　題(Na)　的(DE)　過程(Na)　中(Ng)　對(P)　我們(Nh)　的(DE)　世界(Nc)　現況(Na)　建立(VC)　基本(A)　認識(Nv)　。(PERIODCATEGORY)　智慧(Na)　基地(Nc)　－(DASHCATEGORY)　國網(Nc)　中心(Nc)　展出(VC)　「(PARENTHESISCATEGORY)　搜救犬(Na)　日常(Nd)　訓練(Na)　成果(Na)　三維(Na)　虛擬(VC)　導覽(VC)　系統(Na)　」(PARENTHESISCATEGORY)　，(COMMACATEGORY)　展現(VC)　搜救犬(Na)　中心(Nc)　的(DE)　全方位(Na)　實境(Na)　景觀(Na)　，(COMMACATEGORY)　可(D)　作為(VG)　訓練(Na)　領犬員(Na)　及(Caa)　搜救犬(Na)　之(DE)　參考(Na)　，(COMMACATEGORY)　也(D)　讓(VL)　民眾(Na)　認識(VJ)　犬隻(Na)　訓練(Na)　過程(Na)　和(Caa)　了解(VK)　搜救(Nv)　任務(Na)　。(PERIODCATEGORY)　國研院(Nc)　近年(Nd)　來(Ng)　積極(VH)　把(P)　國家級(Na)　實驗(Nv)　研究(Nv)　單位(Na)　開發(VC)　的(DE)　尖端(A)　科技(Na)　，(COMMACATEGORY)　轉化為(VG)　讓(VL)　中小學生(Na)　都(D)　能(D)　有(V_2)　基本(A)　認識(VJ)　的(DE)　科普(Na)　展覽(VC)　與(Caa)　活動(Na)　，(COMMACATEGORY)　希望(VK)　能(D)　幫助(VC)　中小學生(Na)　建立(VC)　對(P)　科學(Na)　的(DE)　興趣(Na)　，(COMMACATEGORY)　並(Cbb)　培養(VC)　科學(Na)　素養(Na)　。(PERIODCATEGORY)　「(PARENTHESISCATEGORY)　科學家(Na)　的(DE)　秘密(VH)　基地(Nc)　」(PARENTHESISCATEGORY)　會(D)　定期(D)　進行(VC)　展品(Na)　更新(VC)　，(COMMACATEGORY)　持續(VL)　提供(VD)　創新(VC)　的(DE)　國家(Na)　研究(Nv)　成果(Na)　，(COMMACATEGORY)　歡迎(VJ)　民眾(Na)　來(VA)　科教館(Nc)　8樓(Nc)　探索(VE)　尖端(A)　科技(Na)　研究(Nv)　的(DE)　有趣(VH)　之(DE)　處(Na)　。(PERIODCATEGORY)\n",
            "NerToken(word='國科會', ner='ORG', idx=(0, 3))\n",
            "NerToken(word='國家實驗研究院', ner='ORG', idx=(6, 13))\n",
            "NerToken(word='國家太空中心', ner='ORG', idx=(14, 20))\n",
            "NerToken(word='國立臺灣科學教育館', ner='ORG', idx=(22, 31))\n",
            "NerToken(word='2023年3月', ner='DATE', idx=(51, 58))\n",
            "NerToken(word='13萬', ner='CARDINAL', idx=(75, 78))\n",
            "NerToken(word='今（4/16）日', ner='DATE', idx=(84, 92))\n",
            "NerToken(word='8', ner='CARDINAL', idx=(167, 168))\n",
            "NerToken(word='三', ner='CARDINAL', idx=(199, 200))\n",
            "NerToken(word='兩', ner='CARDINAL', idx=(208, 209))\n",
            "NerToken(word='國家實驗動物中心', ner='ORG', idx=(231, 239))\n",
            "NerToken(word='國家地震工程研究中心', ner='ORG', idx=(241, 251))\n",
            "NerToken(word='國震中心', ner='ORG', idx=(252, 256))\n",
            "NerToken(word='台灣海洋科技研究中心', ner='ORG', idx=(258, 268))\n",
            "NerToken(word='海洋中心', ner='ORG', idx=(269, 273))\n",
            "NerToken(word='生物模式中心', ner='ORG', idx=(289, 295))\n",
            "NerToken(word='臺灣', ner='GPE', idx=(673, 675))\n",
            "NerToken(word='300米', ner='QUANTITY', idx=(722, 726))\n",
            "NerToken(word='三', ner='CARDINAL', idx=(933, 934))\n",
            "NerToken(word='第一', ner='ORDINAL', idx=(952, 954))\n",
            "NerToken(word='科政中心', ner='ORG', idx=(1071, 1075))\n",
            "NerToken(word='一', ner='CARDINAL', idx=(1078, 1079))\n",
            "NerToken(word='三', ner='CARDINAL', idx=(1089, 1090))\n",
            "NerToken(word='國研院', ner='ORG', idx=(1241, 1244))\n",
            "NerToken(word='8', ner='CARDINAL', idx=(1359, 1360))\n",
            "\n",
            "為使聽障者享有更無礙的聲音感受，國立陽明交通大學、振興醫院與國家實驗研究院國家儀器科技研究中心（國研院國儀中心）合作，成功開發出「微型法布里–珀羅光纖光學麥克風」（Miniaturized Fabry-Perot fiber-optic microphone based on capillary tube and hydrogel diaphragm），解決麥克風受磁場干擾之問題，其研究成果榮登光學領域頂級期刊《Optics & Laser Technology》。此新型微米級光纖麥克風結構簡單、成本低、訊號穩定、尺寸微小，整個麥克風的尺寸如同一根頭髮，而靈敏度比現有技術提升了約37%，能更細微地捕捉聲音變化，同時可以偵測更高頻的聲音，並且兼顧了輕薄微小的功能，未來可應用於穿戴式裝置上。\n",
            "麥克風是一種將聲音轉換成電子訊號的換能器，在生物醫學影像、語音互動系統和助聽器等應用中發揮著至關重要的作用。然而一般麥克風容易受到強電磁場或是射頻干擾，也有電噪聲過大、靈敏度較低的問題。此次發表之新型微米級光纖麥克風，是利用光纖與光學元件來檢測聲波引起的變化，並將這些變化轉換為可測量的光信號。當聲音壓力作用在麥克風的膜片上，造成膜片的變形或振動，改變了光的干涉條紋，藉由此變化轉換為電子信號來偵測聲音。國立陽明交通大學生物醫學工程學系劉承揚教授、振興醫院耳鼻喉部力博宏主任醫師與國研院國儀中心協力合作，首先由力博宏主任醫師發想光纖麥克風的構造概念，接著由國儀中心協助薄膜（麥克風的膜片）製程與檢測，並由劉承揚教授團隊將光纖與薄膜結合，再進行測試與臨床驗證，最終成功開發出新型微米級光纖麥克風。整個麥克風的尺寸如同一根頭髮，同時具有優異的聲音感測靈敏度和穩定性。新型微米級光纖麥克風由於不含金屬材料，因此不受電磁干擾，適用於助聽器和人工電子耳，使用者可以自由進入強電磁場環境而不必擔心噪音的產生，在光聲成像、健康監測、無損檢測、醫學臨床等應用方面亦有具有巨大的商業潛力。\n",
            "國研院國儀中心薄膜製程開發技術之前亦曾協助劉承揚教授團隊成功用蜘蛛絲製成光纖感測器，精準量測糖尿病患者血糖，近期更往高性能半導體材料應用發展。此次協助光纖薄膜製作，再次證明國儀中心是學術界挑戰世界頂尖科技的關鍵夥伴。國儀中心也期待與國內更多學者合作，應用頂尖的薄膜製程開發技術，研發出更多尖端生醫光電醫療器材。\n",
            "為(P)　使(VL)　聽障(VH)　者(Na)　享有(VJ)　更(Dfa)　無礙(VH)　的(DE)　聲音(Na)　感受(Na)　，(COMMACATEGORY)　國立(A)　陽明(Nb)　交通(Na)　大學(Nc)　、(PAUSECATEGORY)　振興(VC)　醫院(Nc)　與(Caa)　國家(Na)　實驗(Na)　研究院(Nc)　國家(Na)　儀器(Na)　科技(Na)　研究(Nv)　中心(Nc)　（(PARENTHESISCATEGORY)　國研院(Nc)　國儀(Na)　中心(Nc)　）(PARENTHESISCATEGORY)　合作(VH)　，(COMMACATEGORY)　成功(VH)　開發出(VC)　「(PARENTHESISCATEGORY)　微型(Na)　法布里(FW)　–(FW)　珀羅(Nb)　光纖(Na)　光學(Na)　麥克風(Na)　」(PARENTHESISCATEGORY)　（(PARENTHESISCATEGORY)　Miniaturized(FW)　 Fabry-Perot(FW)　 fiber-optic(FW)　 microphone(FW)　 based(FW)　 on(FW)　 capillary(FW)　 tube(FW)　 and(FW)　 hydrogel(FW)　 diaphragm(FW)　）(PARENTHESISCATEGORY)　，(COMMACATEGORY)　解決(VC)　麥克風(Na)　受(P)　磁場(Nc)　干擾(VC)　之(DE)　問題(Na)　，(COMMACATEGORY)　其(Nep)　研究(VE)　成果(Na)　榮登(VC)　光學(Na)　領域(Na)　頂級(A)　期刊(Na)　《(PARENTHESISCATEGORY)　Optics(FW)　 & Laser(FW)　 Technology(FW)　》(PARENTHESISCATEGORY)　。(PERIODCATEGORY)　此(Nep)　新型(Na)　微米級(Nf)　光纖(Na)　麥克風(Na)　結構(Na)　簡單(VH)　、(PAUSECATEGORY)　成本(Na)　低(VH)　、(PAUSECATEGORY)　訊號(Na)　穩定(VHC)　、(PAUSECATEGORY)　尺寸(Na)　微小(VH)　，(COMMACATEGORY)　整(Neqa)　個(Nf)　麥克風(Na)　的(DE)　尺寸(Na)　如同(VG)　一(Neu)　根(Nf)　頭髮(Na)　，(COMMACATEGORY)　而(Cbb)　靈敏度(Na)　比(P)　現有(A)　技術(Na)　提升(VC)　了(Di)　約(Da)　37%(Neqa)　，(COMMACATEGORY)　能(D)　更(Dfa)　細微(VH)　地(DE)　捕捉(VC)　聲音(Na)　變化(Na)　，(COMMACATEGORY)　同時(Nd)　可以(D)　偵測(VC)　更(Dfa)　高頻(Na)　的(DE)　聲音(Na)　，(COMMACATEGORY)　並且(Cbb)　兼顧(VC)　了(Di)　輕薄(VH)　微小(VH)　的(DE)　功能(Na)　，(COMMACATEGORY)　未來(Nd)　可(D)　應用(VC)　於(P)　穿戴式(A)　裝置(Na)　上(Ng)　。(PERIODCATEGORY)　\n",
            "(WHITESPACE)　麥克風(Na)　是(SHI)　一(Neu)　種(Nf)　將(P)　聲音(Na)　轉換成(VG)　電子(Na)　訊號(Na)　的(DE)　換能器(Na)　，(COMMACATEGORY)　在(P)　生物(Na)　醫學(Na)　影像(Na)　、(PAUSECATEGORY)　語音(Na)　互動(VA)　系統(Na)　和(Caa)　助聽器(Na)　等(Cab)　應用(Na)　中(Ng)　發揮(VJ)　著(Di)　至關(Dfa)　重要(VH)　的(DE)　作用(Na)　。(PERIODCATEGORY)　然而(Cbb)　一般(A)　麥克風(Na)　容易(VH)　受到(VJ)　強(VH)　電磁場(Nc)　或是(Caa)　射頻(Na)　干擾(VC)　，(COMMACATEGORY)　也(D)　有(V_2)　電噪聲(Na)　過(Dfa)　大(VH)　、(PAUSECATEGORY)　靈敏度(Na)　較(Dfa)　低(VH)　的(DE)　問題(Na)　。(PERIODCATEGORY)　此(Nep)　次(Nf)　發表(VC)　之(DE)　新型(Na)　微米級(Nf)　光纖(Na)　麥克風(Na)　，(COMMACATEGORY)　是(SHI)　利用(VC)　光纖(Na)　與(Caa)　光學(Na)　元件(Na)　來(D)　檢測(VE)　聲波(Na)　引起(VC)　的(DE)　變化(Na)　，(COMMACATEGORY)　並(Cbb)　將(P)　這些(Neqa)　變化(Na)　轉換為(VG)　可(D)　測量(VC)　的(DE)　光信號(Na)　。(PERIODCATEGORY)　當(P)　聲音(Na)　壓力(Na)　作用(Na)　在(P)　麥克風(Na)　的(DE)　膜片(Na)　上(Ncd)　，(COMMACATEGORY)　造成(VK)　膜片(Na)　的(DE)　變形(VH)　或(Caa)　振動(VAC)　，(COMMACATEGORY)　改變(VC)　了(Di)　光(Na)　的(DE)　干涉(Nv)　條紋(Na)　，(COMMACATEGORY)　藉由(P)　此(Nep)　變化(Na)　轉換為(VG)　電子(Na)　信號(Na)　來(D)　偵測(VC)　聲音(Na)　。(PERIODCATEGORY)　國立(A)　陽明(Nb)　交通(Na)　大學(Nc)　生物(Na)　醫學(Na)　工程(Na)　學系(Na)　劉承揚(Nb)　教授(Na)　、(PAUSECATEGORY)　振興(VC)　醫院(Nc)　耳鼻喉部(Nc)　力博宏(Nb)　主任(Na)　醫師(Na)　與(Caa)　國研院(Nc)　國儀(Na)　中心(Nc)　協力(VH)　合作(VH)　，(COMMACATEGORY)　首先(D)　由(P)　力博宏(Nb)　主任(Na)　醫師(Na)　發想(VE)　光纖(Na)　麥克風(Na)　的(DE)　構造(Na)　概念(Na)　，(COMMACATEGORY)　接著(D)　由(P)　國儀(Nb)　中心(Nc)　協助(VC)　薄膜(Na)　（(PARENTHESISCATEGORY)　麥克風(Na)　的(DE)　膜片(Na)　）(PARENTHESISCATEGORY)　製程(Na)　與(Caa)　檢測(VE)　，(COMMACATEGORY)　並(Cbb)　由(P)　劉承揚(Nb)　教授(Na)　團隊(Na)　將(P)　光纖(Na)　與(Caa)　薄膜(Na)　結合(VHC)　，(COMMACATEGORY)　再(D)　進行(VC)　測試(VC)　與(Caa)　臨床(A)　驗證(VE)　，(COMMACATEGORY)　最終(D)　成功(VH)　開發出(VC)　新型(Na)　微米級(Na)　光纖(Na)　麥克風(Na)　。(PERIODCATEGORY)　整(Neqa)　個(Nf)　麥克風(Na)　的(DE)　尺寸(Na)　如同(VG)　一(Neu)　根(Nf)　頭髮(Na)　，(COMMACATEGORY)　同時(Nd)　具有(VJ)　優異(VH)　的(DE)　聲音(Na)　感測(Nv)　靈敏度(Na)　和(Caa)　穩定性(Na)　。(PERIODCATEGORY)　新型(Na)　微米級(Nf)　光纖(Na)　麥克風(Na)　由於(Cbb)　不(D)　含(VC)　金屬(Na)　材料(Na)　，(COMMACATEGORY)　因此(Cbb)　不(D)　受(P)　電磁(Na)　干擾(VC)　，(COMMACATEGORY)　適用(VJ)　於(P)　助聽器(Na)　和(Caa)　人工(Na)　電子(Na)　耳(Na)　，(COMMACATEGORY)　使用(VC)　者(Na)　可以(D)　自由(VH)　進入(VCL)　強(VH)　電磁場(Nc)　環境(Na)　而(Cbb)　不必(D)　擔心(VK)　噪音(Na)　的(DE)　產生(Nv)　，(COMMACATEGORY)　在(P)　光聲(Na)　成像(VH)　、(PAUSECATEGORY)　健康(VH)　監測(VC)　、(PAUSECATEGORY)　無損(VJ)　檢測(VE)　、(PAUSECATEGORY)　醫學(Na)　臨床(A)　等(Cab)　應用(Na)　方面(Na)　亦(D)　有(V_2)　具有(VJ)　巨大(VH)　的(DE)　商業(Na)　潛力(Na)　。(PERIODCATEGORY)　\n",
            "(WHITESPACE)　國研院(Nc)　國儀(Na)　中心(Nc)　薄膜(Na)　製程(Na)　開發(VC)　技術(Na)　之前(Nd)　亦(D)　曾(D)　協助(VC)　劉承揚(Nb)　教授(Na)　團隊(Na)　成功(VH)　用(P)　蜘蛛絲(Na)　製成(VG)　光纖(Na)　感測器(Na)　，(COMMACATEGORY)　精準(VH)　量測(VC)　糖尿病(Na)　患者(Na)　血糖(Na)　，(COMMACATEGORY)　近期(Nd)　更(Dfa)　往(P)　高性能(A)　半導體(Na)　材料(Na)　應用(Na)　發展(VC)　。(PERIODCATEGORY)　此(Nep)　次(Nf)　協助(VC)　光纖(Na)　薄膜(Na)　製作(Nv)　，(COMMACATEGORY)　再次(D)　證明(VE)　國儀(Na)　中心(Nc)　是(SHI)　學術界(Nc)　挑戰(VC)　世界(Nc)　頂尖(VH)　科技(Na)　的(DE)　關鍵(Na)　夥伴(Na)　。(PERIODCATEGORY)　國儀(Na)　中心(Nc)　也(D)　期待(VK)　與(P)　國內(Nc)　更多(Neqa)　學者(Na)　合作(VH)　，(COMMACATEGORY)　應用(Na)　頂尖(VH)　的(DE)　薄膜(Na)　製程(Na)　開發(VC)　技術(Na)　，(COMMACATEGORY)　研發出(VC)　更多(Neqa)　尖端(A)　生醫(Na)　光電(Na)　醫療(VC)　器材(Na)　。(PERIODCATEGORY)\n",
            "NerToken(word='國立陽明交通大學', ner='ORG', idx=(16, 24))\n",
            "NerToken(word='37%', ner='PERCENT', idx=(293, 296))\n",
            "NerToken(word='國立陽明交通大學生物醫學工程學系', ner='ORG', idx=(551, 567))\n",
            "NerToken(word='劉承揚', ner='PERSON', idx=(567, 570))\n",
            "NerToken(word='振興醫院', ner='ORG', idx=(573, 577))\n",
            "NerToken(word='力博宏', ner='PERSON', idx=(581, 584))\n",
            "NerToken(word='國研院國儀中心', ner='ORG', idx=(589, 596))\n",
            "NerToken(word='國儀中心', ner='ORG', idx=(627, 631))\n",
            "NerToken(word='國研院國儀中心', ner='ORG', idx=(835, 842))\n",
            "NerToken(word='劉承揚', ner='PERSON', idx=(856, 859))\n",
            "NerToken(word='國儀中心', ner='ORG', idx=(921, 925))\n",
            "NerToken(word='國儀中心', ner='ORG', idx=(943, 947))\n",
            "\n",
            "對於小油坑失火事件，陽明山國家公園管理處表示，初步研判此次火災有可能是國家實驗研究院國家高速網路與計算中心（國研院國網中心）之設備造成，國研院院長蔡宏營對此表示，國研院將慎重面對，如果最後調查結果確實是國研院國網中心之設備造成，國研院將承擔相關責任，絕不卸責。國研院已啟動內部調查，並將全力配合消防單位之外部調查，釐清起火原因；同時亦將仔細盤點現有設備，預防類似事件再度發生。國研院表示，國研院國網中心在小油坑地區安裝空氣品質感測器，並透過用於防災的無線電波段Band 20，將感測資料傳輸回國網中心主機，驗證Band 20在沒有商用無線網路5G、6G及惡劣環境中，亦可傳輸資料，對於提升防救災工作的通訊傳輸能力有很大幫助；同時藉由空氣品質感測器，收集陽明山區域過去1年8個月的PM 2.5資料，在民生公共物聯網上對外公開，提供各界使用，幫助學研界增加對陽明山地區空氣污染情形的了解。\n",
            "對於(P)　小油坑(Nc)　失火(VH)　事件(Na)　，(COMMACATEGORY)　陽明山(Nc)　國家(Na)　公園(Nc)　管理處(Nc)　表示(VE)　，(COMMACATEGORY)　初步(D)　研判(VE)　此(Nep)　次(Nf)　火災(Na)　有(V_2)　可能(D)　是(SHI)　國家(Na)　實驗(Na)　研究院(Nc)　國家(Na)　高速(VH)　網路(Na)　與(Caa)　計算(VC)　中心(Nc)　（(PARENTHESISCATEGORY)　國研院(Nc)　國網(Na)　中心(Nc)　）(PARENTHESISCATEGORY)　之(DE)　設備(Na)　造成(VK)　，(COMMACATEGORY)　國研院(Nc)　院長(Na)　蔡宏營(Nb)　對(P)　此(Nep)　表示(VE)　，(COMMACATEGORY)　國研院(Nc)　將(D)　慎重(VH)　面對(VC)　，(COMMACATEGORY)　如果(Cbb)　最後(Nd)　調查(Nv)　結果(Na)　確實(D)　是(SHI)　國研院(Nc)　國網(Na)　中心(Nc)　之(DE)　設備(Na)　造成(VK)　，(COMMACATEGORY)　國研院(Nc)　將(D)　承擔(VC)　相關(VH)　責任(Na)　，(COMMACATEGORY)　絕不(D)　卸責(VA)　。(PERIODCATEGORY)　國研院(Nc)　已(D)　啟動(VC)　內部(Ncd)　調查(Nv)　，(COMMACATEGORY)　並(Cbb)　將(D)　全力(D)　配合(VC)　消防(A)　單位(Na)　之(DE)　外部(Ncd)　調查(Nv)　，(COMMACATEGORY)　釐清(VC)　起火(VH)　原因(Na)　；(SEMICOLONCATEGORY)　同時(Nd)　亦(D)　將(D)　仔細(VH)　盤點(VC)　現有(A)　設備(Na)　，(COMMACATEGORY)　預防(VE)　類似(VG)　事件(Na)　再度(D)　發生(VJ)　。(PERIODCATEGORY)　國研院(Nc)　表示(VE)　，(COMMACATEGORY)　國研院(Nc)　國網(Na)　中心(Nc)　在(P)　小油坑(Nc)　地區(Nc)　安裝(VC)　空氣(Na)　品質(Na)　感測器(Na)　，(COMMACATEGORY)　並(Cbb)　透過(P)　用於(VCL)　防災(VH)　的(DE)　無線電(Na)　波段(Na)　Band(FW)　 20(FW)　，(COMMACATEGORY)　將(P)　感測(Nv)　資料(Na)　傳輸(VC)　回(VCL)　國網(Na)　中心(Nc)　主機(Na)　，(COMMACATEGORY)　驗證(VE)　Band(FW)　 20(FW)　在(P)　沒有(VJ)　商用(A)　無線(A)　網路(Na)　5G(Neu)　、(PAUSECATEGORY)　6G(Neu)　及(Caa)　惡劣(VH)　環境(Na)　中(Ng)　，(COMMACATEGORY)　亦(D)　可(D)　傳輸(VC)　資料(Na)　，(COMMACATEGORY)　對於(P)　提升(VC)　防救災(VA)　工作(Na)　的(DE)　通訊(Na)　傳輸(VC)　能力(Na)　有(V_2)　很(Dfa)　大(VH)　幫助(VC)　；(SEMICOLONCATEGORY)　同時(Nd)　藉由(P)　空氣(Na)　品質(Na)　感測器(Na)　，(COMMACATEGORY)　收集(VC)　陽明山(Nc)　區域(Nc)　過去(Nd)　1(Neu)　年(Nf)　8(Neu)　個(Nf)　月(Na)　的(DE)　PM 2.5(FW)　資料(Na)　，(COMMACATEGORY)　在(P)　民生(Na)　公共(A)　物聯網(Na)　上(Ng)　對(P)　外(Ncd)　公開(VHC)　，(COMMACATEGORY)　提供(VD)　各界(Na)　使用(VC)　，(COMMACATEGORY)　幫助(VC)　學研界(Nc)　增加(VHC)　對(P)　陽明山(Nc)　地區(Nc)　空氣(Na)　污染(VC)　情形(Na)　的(DE)　了解(Nv)　。(PERIODCATEGORY)\n",
            "NerToken(word='國研院', ner='ORG', idx=(68, 71))\n",
            "NerToken(word='蔡宏營', ner='PERSON', idx=(73, 76))\n",
            "NerToken(word='國研院', ner='ORG', idx=(81, 84))\n",
            "NerToken(word='國研院', ner='ORG', idx=(114, 117))\n",
            "NerToken(word='國研院', ner='ORG', idx=(130, 133))\n",
            "NerToken(word='國研院', ner='ORG', idx=(188, 191))\n",
            "NerToken(word='陽明山', ner='LOC', idx=(326, 329))\n",
            "NerToken(word='過去1年8個月', ner='DATE', idx=(331, 338))\n",
            "NerToken(word='陽明山', ner='LOC', idx=(377, 380))\n",
            "\n",
            "為偵測大屯火山活動噴發氣體與空氣品質，國家實驗研究院國家高速網路與計算中心（國研院國網中心）利用空氣品質感測器，在小油坑地區蒐集大屯山地區的空氣品質資料。\n",
            "小油坑地區位於陽明山國家公園管理範圍內，本計畫於2022年正式向陽明山國家公園管理處提出申請，並獲得同意，程序合法合規，且已於2024年底結束。\n",
            "本計畫係委託廠商架設空氣品質感測器，透過網路線即時回傳空品數據。電力來源是透過太陽能供電系統，保障在偏遠無電區域仍可持續運作。\n",
            "承包廠商以太陽能桿提供空氣品質感測器所需電力，下圖即為太陽能板發電與儲電設備及空氣品質感測器。太陽能桿與空品感測器皆為合法合規之設備，並經測試與驗證後才進行部署。\n",
            "為(P)　偵測(VC)　大屯(Nc)　火山(Na)　活動(Na)　噴發(Nv)　氣體(Na)　與(Caa)　空氣(Na)　品質(Na)　，(COMMACATEGORY)　國家(Na)　實驗(Na)　研究院(Nc)　國家(Na)　高速(VH)　網路(Na)　與(Caa)　計算(VC)　中心(Nc)　（(PARENTHESISCATEGORY)　國研院(Nc)　國網(Na)　中心(Nc)　）(PARENTHESISCATEGORY)　利用(VC)　空氣(Na)　品質(Na)　感測器(Na)　，(COMMACATEGORY)　在(P)　小油坑(Nc)　地區(Nc)　蒐集(VC)　大屯山(Nc)　地區(Nc)　的(DE)　空氣(Na)　品質(Na)　資料(Na)　。(PERIODCATEGORY)　\n",
            "(WHITESPACE)　小油坑(Nc)　地區(Nc)　位於(VCL)　陽明山(Nc)　國家(Na)　公園(Nc)　管理(VC)　範圍(Na)　內(Ncd)　，(COMMACATEGORY)　本(Nes)　計畫(Na)　於(P)　2022年(Nd)　正式(VH)　向(P)　陽明山(Nc)　國家(Na)　公園(Nc)　管理處(Nc)　提出(VC)　申請(VF)　，(COMMACATEGORY)　並(Cbb)　獲得(VJ)　同意(VK)　，(COMMACATEGORY)　程序(Na)　合法(VH)　合規(VH)　，(COMMACATEGORY)　且(Cbb)　已(D)　於(P)　2024年(Neu)　底(Ng)　結束(VHC)　。(PERIODCATEGORY)　\n",
            "(WHITESPACE)　本(Nes)　計畫(Na)　係(VG)　委託(VF)　廠商(Na)　架設(VC)　空氣(Na)　品質(Na)　感測器(Na)　，(COMMACATEGORY)　透過(P)　網路線(Na)　即時(D)　回傳(VC)　空品(Na)　數據(Na)　。(PERIODCATEGORY)　電力(Na)　來源(Na)　是(SHI)　透過(P)　太陽能(Na)　供電(VF)　系統(Na)　，(COMMACATEGORY)　保障(VC)　在(P)　偏遠(VH)　無(VJ)　電(Na)　區域(Nc)　仍(D)　可(D)　持續(VL)　運作(VA)　。(PERIODCATEGORY)　\n",
            "(WHITESPACE)　承包(Nv)　廠商(Na)　以(P)　太陽能(Na)　桿(Na)　提供(VD)　空氣(Na)　品質(Na)　感測器(Na)　所(D)　需(VK)　電力(Na)　，(COMMACATEGORY)　下(Ncd)　圖(Na)　即為(VG)　太陽能板(Na)　發電(VA)　與(Caa)　儲電(VC)　設備(Na)　及(Caa)　空氣(Na)　品質(Na)　感測器(Na)　。(PERIODCATEGORY)　太陽能(Na)　桿(Na)　與(Caa)　空品(Na)　感測器(Na)　皆(D)　為(VG)　合法(VH)　合規(VA)　之(DE)　設備(Na)　，(COMMACATEGORY)　並(Cbb)　經(P)　測試(VC)　與(Caa)　驗證(VE)　後(Ng)　才(Da)　進行(VC)　部署(VC)　。(PERIODCATEGORY)\n",
            "NerToken(word='大屯火山', ner='LOC', idx=(3, 7))\n",
            "NerToken(word='國家實驗研究院國家高速網路與計算中心', ner='ORG', idx=(19, 37))\n",
            "NerToken(word='國研院國網中心', ner='ORG', idx=(38, 45))\n",
            "NerToken(word='大屯山', ner='LOC', idx=(64, 67))\n",
            "NerToken(word='小油坑', ner='FAC', idx=(78, 81))\n",
            "NerToken(word='陽明山國家公園', ner='FAC', idx=(85, 92))\n",
            "NerToken(word='2022年', ner='DATE', idx=(102, 107))\n",
            "NerToken(word='陽明山國家公園管理處', ner='ORG', idx=(110, 120))\n",
            "NerToken(word='2024年底', ner='DATE', idx=(141, 147))\n",
            "\n",
            "國科會轄下之國家實驗研究院（國研院）與法國原子能暨替代能源總署（CEA）轄下之電子暨資訊技術研究室（CEA-Leti），於臺灣時間4月10日簽訂合作備忘錄，未來將以雙邊工作坊深化技術交流。法國國家健康與醫學研究院（Inserm）則與國研院於4月7日在巴黎辦理「臺法雙邊器官晶片科學論壇」，期能結合法國在基礎醫學實驗與我國在半導體、儀器技術的專長，建立實質的合作研究計畫，並將我國的生物晶片技術推展至歐洲。法國電子暨資訊技術研究室是法國最重要的電子資訊研究單位，專精於微電子和奈米技術應用。國研院與法國電子暨資訊技術研究室未來將在半導體、生醫、光學技術等領域共同舉辦工作坊或研討會，並進行人才交流互訪，期能促進雙方實質技術交流與研究合作。法國國家健康與醫學研究院則是法國首屈一指的生醫研究單位，此次與國研院邀集臺法雙方各12個具藥物開發應用潛力、且已完成概念性驗證或雛型品開發之研究團隊，共同辦理「臺法雙邊器官晶片科學論壇」，就生物材料（Biomaterial）、感測器與影像（Sensor and Imaging）及多功能器官晶片（Multicomponent & Multifunctional OoC）三大主題交流討論，期望透過成果媒合，促成合作研究的機會，加成雙方研發優勢。器官晶片是國科會與法國高等教育暨研究部於2024年臺法科學研究會議中列出的合作項目之一，是全球最受矚目的活體動物替代方法，結合了3D立體細胞培養、微流道、生醫感測器及人工智慧等技術，在實驗室重建器官內部的結構與環境。若模擬人體健康器官功能進行試藥，可用於藥物毒性或副作用的測試；若模擬生病的器官，則有助於藥物功效的測試與篩選；若能建立病人的器官晶片，更有機會直接運用於個人化醫療策略的輔助評估。此次參與論壇的臺灣團隊來自國科會生科處專案補助之「動物實驗替代科技研發計畫」，以及國研院的「器官晶片多元驗證平台」，包括陽明交通大學、清華大學、臺北醫學大學、國家衛生研究院、長庚醫院，以及國研院國家生物模式中心（原國家實驗動物中心）與國家儀器科技研究中心（原台灣儀器科技研究中心）的研究團隊。論壇結束後，國研院蔡宏營院長並率隊參訪法國巴黎－薩克雷大學（UPSaclay）及法國國家科學研究院（CNRS），期能建立臺法雙方更緊密、更全面的合作關係，藉由實習生交流、大型研究設施共享、合作執行研究計畫等，提升臺法雙方在生醫、高速計算、儀器科技領域的科技研發量能。\n",
            "國科會(Nc)　轄下(Nc)　之(DE)　國家(Na)　實驗(Na)　研究院(Nc)　（(PARENTHESISCATEGORY)　國研院(Nc)　）(PARENTHESISCATEGORY)　與(P)　法國(Nc)　原子能(Na)　暨(Caa)　替代(VJ)　能源(Na)　總署(Nc)　（(PARENTHESISCATEGORY)　CEA(FW)　）(PARENTHESISCATEGORY)　轄下(Nc)　之(DE)　電子(Na)　暨(Caa)　資訊(Na)　技術(Na)　研究室(Nc)　（(PARENTHESISCATEGORY)　CEA-Leti(FW)　）(PARENTHESISCATEGORY)　，(COMMACATEGORY)　於(P)　臺灣(Nc)　時間(Na)　4月(Nd)　10日(Nd)　簽訂(VC)　合作(VH)　備忘錄(Na)　，(COMMACATEGORY)　未來(Nd)　將(D)　以(P)　雙邊(A)　工作坊(Nc)　深化(VHC)　技術(Na)　交流(VH)　。(PERIODCATEGORY)　法國(Nc)　國家(Na)　健康(VH)　與(Caa)　醫學(Na)　研究院(Nc)　（(PARENTHESISCATEGORY)　Inserm(FW)　）(PARENTHESISCATEGORY)　則(D)　與(P)　國研院(Nc)　於(P)　4月(Nd)　7日(Nd)　在(P)　巴黎(Nc)　辦理(VC)　「(PARENTHESISCATEGORY)　臺(Nc)　法(Nc)　雙邊(A)　器官(Na)　晶片(Na)　科學(Na)　論壇(Na)　」(PARENTHESISCATEGORY)　，(COMMACATEGORY)　期(VK)　能(D)　結合(VHC)　法國(Nc)　在(P)　基礎(Na)　醫學(Na)　實驗(Na)　與(Caa)　我國(Nc)　在(P)　半導體(Na)　、(PAUSECATEGORY)　儀器(Na)　技術(Na)　的(DE)　專長(Na)　，(COMMACATEGORY)　建立(VC)　實質(Na)　的(DE)　合作(VH)　研究(Nv)　計畫(Na)　，(COMMACATEGORY)　並(Cbb)　將(P)　我國(Nc)　的(DE)　生物(Na)　晶片(Na)　技術(Na)　推展(VC)　至(P)　歐洲(Nc)　。(PERIODCATEGORY)　法國(Nc)　電子(Na)　暨(Caa)　資訊(Na)　技術(Na)　研究室(Nc)　是(SHI)　法國(Nc)　最(Dfa)　重要(VH)　的(DE)　電子(Na)　資訊(Na)　研究(VE)　單位(Na)　，(COMMACATEGORY)　專精(VJ)　於(P)　微電子(Na)　和(Caa)　奈米(Na)　技術(Na)　應用(Na)　。(PERIODCATEGORY)　國研院(Nc)　與(Caa)　法國(Nc)　電子(Na)　暨(Caa)　資訊(Na)　技術(Na)　研究室(Nc)　未來(Nd)　將(D)　在(P)　半導體(Na)　、(PAUSECATEGORY)　生醫(Na)　、(PAUSECATEGORY)　光學(Na)　技術(Na)　等(Cab)　領域(Na)　共同(A)　舉辦(VC)　工作坊(Nc)　或(Caa)　研討會(Na)　，(COMMACATEGORY)　並(Cbb)　進行(VC)　人才(Na)　交流(VH)　互訪(VC)　，(COMMACATEGORY)　期(VK)　能(D)　促進(VK)　雙方(Nh)　實質(Na)　技術(Na)　交流(VH)　與(Caa)　研究(Nv)　合作(VH)　。(PERIODCATEGORY)　法國(Nc)　國家(Na)　健康(VH)　與(Caa)　醫學(Na)　研究院(Nc)　則(D)　是(SHI)　法國(Nc)　首屈一指(VH)　的(DE)　生醫(Na)　研究(VE)　單位(Na)　，(COMMACATEGORY)　此(Nep)　次(Nf)　與(Caa)　國研院(Nc)　邀集(VC)　臺(Nc)　法(Nc)　雙方(Nh)　各(Nes)　12(Neu)　個(Nf)　具(VJ)　藥物(Na)　開發(VC)　應用(Na)　潛力(Na)　、(PAUSECATEGORY)　且(Cbb)　已(D)　完成(VC)　概念性(Na)　驗證(VE)　或(Caa)　雛型品(Na)　開發(VC)　之(DE)　研究(Nv)　團隊(Na)　，(COMMACATEGORY)　共同(A)　辦理(VC)　「(PARENTHESISCATEGORY)　臺(Nc)　法(Nc)　雙邊(A)　器官(Na)　晶片(Na)　科學(Na)　論壇(Na)　」(PARENTHESISCATEGORY)　，(COMMACATEGORY)　就(P)　生物(Na)　材料(Na)　（(PARENTHESISCATEGORY)　Biomaterial(FW)　）(PARENTHESISCATEGORY)　、(PAUSECATEGORY)　感測器(Na)　與(Caa)　影像(Na)　（(PARENTHESISCATEGORY)　Sensor(FW)　 and(FW)　 Imaging(FW)　）(PARENTHESISCATEGORY)　及(Caa)　多功能(VH)　器官(Na)　晶片(Na)　（(PARENTHESISCATEGORY)　Multicomponent(FW)　 & (FW)　Multifunctional(FW)　 OoC(FW)　）(PARENTHESISCATEGORY)　三(Neu)　大(VH)　主題(Na)　交流(VH)　討(VE)　論(VE)　，(COMMACATEGORY)　期望(VK)　透過(P)　成果(Na)　媒合(VC)　，(COMMACATEGORY)　促成(VK)　合作(VH)　研究(VE)　的(DE)　機會(Na)　，(COMMACATEGORY)　加成(VA)　雙方(Nh)　研發(VC)　優勢(Na)　。(PERIODCATEGORY)　器官(Na)　晶片(Na)　是(SHI)　國科會(Nc)　與(Caa)　法國(Nc)　高等(A)　教育(Na)　暨(Caa)　研究部(Nc)　於(P)　2024年(Neu)　臺(Nc)　法(Nc)　科學(Na)　研究(Nv)　會議(Na)　中(Ng)　列出(VC)　的(DE)　合作(VH)　項目(Na)　之(DE)　一(Neu)　，(COMMACATEGORY)　是(SHI)　全球(Nc)　最(Dfa)　受(VJ)　矚目(VB)　的(DE)　活體(Na)　動物(Na)　替代(VJ)　方法(Na)　，(COMMACATEGORY)　結合(VHC)　了(Di)　3D(Neu)　立體(VH)　細胞(Na)　培養(VC)　、(PAUSECATEGORY)　微流道(Na)　、(PAUSECATEGORY)　生醫(Na)　感測器(Na)　及(Caa)　人工(Na)　智慧(Na)　等(Cab)　技術(Na)　，(COMMACATEGORY)　在(P)　實驗室(Nc)　重建(VC)　器官(Na)　內部(Ncd)　的(DE)　結構(Na)　與(Caa)　環境(Na)　。(PERIODCATEGORY)　若(Cbb)　模擬(VC)　人體(Na)　健康(VH)　器官(Na)　功能(Na)　進行(VC)　試藥(Na)　，(COMMACATEGORY)　可(D)　用於(VCL)　藥物(Na)　毒性(Na)　或(Caa)　副作用(Na)　的(DE)　測試(Na)　；(SEMICOLONCATEGORY)　若(Cbb)　模擬(VC)　生病(VH)　的(DE)　器官(Na)　，(COMMACATEGORY)　則(D)　有助於(VK)　藥物(Na)　功效(Na)　的(DE)　測試(Na)　與(Caa)　篩選(VC)　；(SEMICOLONCATEGORY)　若(Cbb)　能(D)　建立(VC)　病人(Na)　的(DE)　器官(Na)　晶片(Na)　，(COMMACATEGORY)　更(Dfa)　有(V_2)　機會(Na)　直接(VH)　運用(VC)　於(P)　個人化(VHC)　醫療(VC)　策略(Na)　的(DE)　輔助(VC)　評估(VE)　。(PERIODCATEGORY)　此(Nep)　次(Nf)　參與(VC)　論壇(Na)　的(DE)　臺灣(Nc)　團隊(Na)　來自(VJ)　國科會(Nc)　生科處(Nc)　專案(Na)　補助(VD)　之(DE)　「(PARENTHESISCATEGORY)　動物(Na)　實驗(Na)　替代(Nv)　科技(Na)　研發(Nv)　計畫(Na)　」(PARENTHESISCATEGORY)　，(COMMACATEGORY)　以及(Caa)　國研院(Nc)　的(DE)　「(PARENTHESISCATEGORY)　器官(Na)　晶片(Na)　多元(VH)　驗證(VE)　平台(Na)　」(PARENTHESISCATEGORY)　，(COMMACATEGORY)　包括(VK)　陽明(Nb)　交通(Na)　大學(Nc)　、(PAUSECATEGORY)　清華(Nb)　大學(Nc)　、(PAUSECATEGORY)　臺北(Nc)　醫學(Na)　大學(Nc)　、(PAUSECATEGORY)　國家(Na)　衛生(Na)　研究院(Nc)　、(PAUSECATEGORY)　長庚(Nb)　醫院(Nc)　，(COMMACATEGORY)　以及(Caa)　國研院(Nc)　國家(Na)　生物(Na)　模式(Na)　中心(Nc)　（(PARENTHESISCATEGORY)　原(A)　國家(Na)　實驗(Nv)　動物(Na)　中心(Nc)　）(PARENTHESISCATEGORY)　與(P)　國家(Na)　儀器(Na)　科技(Na)　研究(Nv)　中心(Nc)　（(PARENTHESISCATEGORY)　原(A)　台灣(Nc)　儀器(Na)　科技(Na)　研究(Nv)　中心(Nc)　）(PARENTHESISCATEGORY)　的(DE)　研究(Nv)　團隊(Na)　。(PERIODCATEGORY)　論壇(Na)　結束(VHC)　後(Ng)　，(COMMACATEGORY)　國研院(Nc)　蔡宏營(Nb)　院長(Na)　並(Cbb)　率隊(VA)　參訪(VC)　法國(Nc)　巴黎(Nc)　－(DASHCATEGORY)　薩克雷(Nb)　大學(Nc)　（(PARENTHESISCATEGORY)　UPSaclay(FW)　）(PARENTHESISCATEGORY)　及(Caa)　法國(Nc)　國家(Na)　科學(Na)　研究院(Nc)　（(PARENTHESISCATEGORY)　CNRS(FW)　）(PARENTHESISCATEGORY)　，(COMMACATEGORY)　期(VK)　能(D)　建立(VC)　臺(Nc)　法(Nc)　雙方(Nh)　更(D)　緊密(VH)　、(PAUSECATEGORY)　更(D)　全面(VH)　的(DE)　合作(VH)　關係(Na)　，(COMMACATEGORY)　藉由(P)　實習生(Na)　交流(VH)　、(PAUSECATEGORY)　大型(A)　研究(Nv)　設施(Na)　共享(VJ)　、(PAUSECATEGORY)　合作(VH)　執行(VC)　研究(Nv)　計畫(Na)　等(Cab)　，(COMMACATEGORY)　提升(VC)　臺(Nc)　法(Nc)　雙方(Nh)　在(P)　生醫(Na)　、(PAUSECATEGORY)　高速(VH)　計算(Nv)　、(PAUSECATEGORY)　儀器(Na)　科技(Na)　領域(Na)　的(DE)　科技(Na)　研發(Nv)　量能(Na)　。(PERIODCATEGORY)\n",
            "NerToken(word='法國', ner='GPE', idx=(19, 21))\n",
            "NerToken(word='臺灣', ner='GPE', idx=(61, 63))\n",
            "NerToken(word='4月10日', ner='DATE', idx=(65, 70))\n",
            "NerToken(word='法國', ner='GPE', idx=(94, 96))\n",
            "NerToken(word='國研院', ner='ORG', idx=(116, 119))\n",
            "NerToken(word='4月7日', ner='DATE', idx=(120, 124))\n",
            "NerToken(word='巴黎', ner='GPE', idx=(125, 127))\n",
            "NerToken(word='法', ner='GPE', idx=(131, 132))\n",
            "NerToken(word='法國', ner='GPE', idx=(148, 150))\n",
            "NerToken(word='歐洲', ner='LOC', idx=(199, 201))\n",
            "NerToken(word='法國', ner='GPE', idx=(202, 204))\n",
            "NerToken(word='法國', ner='GPE', idx=(215, 217))\n",
            "NerToken(word='法國', ner='GPE', idx=(248, 250))\n",
            "NerToken(word='法國', ner='GPE', idx=(318, 320))\n",
            "NerToken(word='法國', ner='GPE', idx=(332, 334))\n",
            "NerToken(word='臺', ner='GPE', idx=(354, 355))\n",
            "NerToken(word='法', ner='GPE', idx=(355, 356))\n",
            "NerToken(word='12', ner='CARDINAL', idx=(359, 361))\n",
            "NerToken(word='臺法', ner='GPE', idx=(398, 400))\n",
            "NerToken(word='三', ner='CARDINAL', idx=(503, 504))\n",
            "NerToken(word='國科會', ner='ORG', idx=(545, 548))\n",
            "NerToken(word='法國高等教育暨研究部', ner='ORG', idx=(549, 559))\n",
            "NerToken(word='2024年', ner='DATE', idx=(560, 565))\n",
            "NerToken(word='國科會生科處', ner='ORG', idx=(750, 756))\n",
            "NerToken(word='國研院', ner='ORG', idx=(778, 781))\n",
            "NerToken(word='陽明交通大學', ner='ORG', idx=(797, 803))\n",
            "NerToken(word='清華大學', ner='ORG', idx=(804, 808))\n",
            "NerToken(word='臺北醫學大學', ner='ORG', idx=(809, 815))\n",
            "NerToken(word='國家衛生研究院', ner='ORG', idx=(816, 823))\n",
            "NerToken(word='長庚醫院', ner='ORG', idx=(824, 828))\n",
            "NerToken(word='國研院國家生物模式中心', ner='ORG', idx=(831, 842))\n",
            "NerToken(word='國家實驗動物中心', ner='ORG', idx=(844, 852))\n",
            "NerToken(word='國家儀器科技研究中心', ner='ORG', idx=(854, 864))\n",
            "NerToken(word='台灣儀器科技研究中心', ner='ORG', idx=(866, 876))\n",
            "NerToken(word='國研院', ner='ORG', idx=(889, 892))\n",
            "NerToken(word='法國巴黎－薩克雷大學', ner='ORG', idx=(902, 912))\n",
            "NerToken(word='法國國家科學研究院', ner='ORG', idx=(923, 932))\n",
            "NerToken(word='臺', ner='GPE', idx=(943, 944))\n",
            "NerToken(word='法', ner='GPE', idx=(944, 945))\n",
            "NerToken(word='臺', ner='GPE', idx=(989, 990))\n",
            "NerToken(word='法', ner='GPE', idx=(990, 991))\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 關鍵詞提取"
      ],
      "metadata": {
        "id": "2OXP6lpLTcvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "nK6oB-lsTfES",
        "outputId": "47bda00a-6644-4369-a7af-4365b618cdab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (2.0.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (4.13.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.1.31)\n",
            "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keybert\n",
            "Successfully installed keybert-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "2ZBru4ZNToEz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ws_zh(text):\n",
        "    words = ws_driver([text])\n",
        "    return words[0]"
      ],
      "metadata": {
        "id": "DBnYZQygTxgj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(tokenizer=ws_zh)"
      ],
      "metadata": {
        "id": "GT78q_KzT3si"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model = KeyBERT()\n",
        "#kw_model = KeyBERT(model='distiluse-base-multilingual-cased-v1')"
      ],
      "metadata": {
        "id": "g1YlgZ1TT8F-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,5):\n",
        "\n",
        "  keywords = kw_model.extract_keywords(dict_list[i][\"content\"],vectorizer=vectorizer)\n",
        "  print(dict_list[i][\"content\"])\n",
        "  print(keywords)"
      ],
      "metadata": {
        "id": "f3M9bU2giPGY",
        "outputId": "447db93f-1477-45b6-af6f-6ba2775b34d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 62.85it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:06<00:00,  6.85s/it]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 220.02it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:05<00:00,  5.13s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "國科會轄下之國家實驗研究院及國家太空中心，與國立臺灣科學教育館合作，辦理「科學家的秘密基地」長期展，自2023年3月開展以來，備受各界好評，已吸引超過13萬人入場參觀。今（4/16）日於更新部分展品後重新開展，希望能幫助參觀民眾透過互動遊戲、親眼觀察與模型展示，習得有趣的科學知識，同時深入認識科研工作。「科學家的秘密基地」位於科教館8樓東南側扇形展場，分為「實驗基地」、「探測基地」和「智慧基地」三區，每一區均包含兩大展示主題。此次更新的是國家生物模式中心（原國家實驗動物中心）、國家地震工程研究中心（國震中心）及台灣海洋科技研究中心（海洋中心）的展品。一進門就看到實驗基地－生物模式中心展出的「貓咪毛色變化的秘密」，這裡展示多基因協同作用的觀念。生物模式中心近年以革新的基因編輯工具「CRISPR/Cas」基因剪刀產製「擬人鼠」動物模式，推進我國精準醫療研究與臨床應用，而科學家進行基因編輯時，須嚴加考慮生物體複雜的基因層次。現場除了深入淺出的科學觀念，也設計了一個基因密碼轉盤遊戲，參觀者可以透過簡單的科普工具，輕鬆學習生物醫學知識。實驗基地－國震中心透過互動展示，介紹房屋的共振原理，包括不同樓高建築物的振動特性，以及地震波在不同地質條件下傳遞所產生的共振效應。現場提供以不同長度的吸管模擬各種樓高的共振教具，讓參觀者能親身體驗結構與地震波共振的現象。同時展示「滾動式隔震平台」，結合電動振動台模擬真實地震波，可清楚觀察放置於隔震裝置上的設備，其振動幅度大幅減少，讓參觀者直觀感受隔震技術的保護效果。接著來到探測基地－海洋中心，這裡展出於臺灣西南海域及南海採集到的海洋浮游動物標本、寫真、採集工具，以及使用海洋中心自製底碇平台記錄之水下300米的海底世界。海洋浮游動物在海洋生態系中扮演著重要而多樣的角色，牠們的種類及數量非常容易受到海洋環境的影響，當環境變動時（如溫度、鹽度、酸鹼度），浮游動物的組成也會隨之改變，間接對魚類、鯨類等各種海洋動物產生連鎖反應。因此浮游動物是一種很好的環境指標生物，可以用來監測海洋環境變遷、預測海洋生態系的改變。展場設置顯微觀察區，讓參觀者一窺這群嬌小但至關重要的小小兵身影，了解牠們是維護海洋生態平衡的關鍵多數。除了更新的三件展品外，探測基地－太空中心展出我國第一顆自製氣象衛星獵風者衛星（Triton）的1：1模型及其元件介紹。獵風者衛星搭載國家太空中心自行研發的「全球導航衛星系統反射訊號接收儀」（GNSS-R），進行海洋風場、海面高度、海氣交互作用、地表含水量、颱風強度預測等研究。智慧基地－科政中心設計出一套多媒體互動遊戲，有三題選擇題，題目出自「PRIDE政策研究指標資料庫」，觀眾只要揮動手臂就可以在螢幕上選擇答案，在答題的過程中對我們的世界現況建立基本認識。智慧基地－國網中心展出「搜救犬日常訓練成果三維虛擬導覽系統」，展現搜救犬中心的全方位實境景觀，可作為訓練領犬員及搜救犬之參考，也讓民眾認識犬隻訓練過程和了解搜救任務。國研院近年來積極把國家級實驗研究單位開發的尖端科技，轉化為讓中小學生都能有基本認識的科普展覽與活動，希望能幫助中小學生建立對科學的興趣，並培養科學素養。「科學家的秘密基地」會定期進行展品更新，持續提供創新的國家研究成果，歡迎民眾來科教館8樓探索尖端科技研究的有趣之處。\n",
            "[('中心', 0.4116), ('地震波', 0.331), ('基地', 0.2928), ('地表', 0.2879), ('地質', 0.2879)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 194.97it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 680.56it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "對於小油坑失火事件，陽明山國家公園管理處表示，初步研判此次火災有可能是國家實驗研究院國家高速網路與計算中心（國研院國網中心）之設備造成，國研院院長蔡宏營對此表示，國研院將慎重面對，如果最後調查結果確實是國研院國網中心之設備造成，國研院將承擔相關責任，絕不卸責。國研院已啟動內部調查，並將全力配合消防單位之外部調查，釐清起火原因；同時亦將仔細盤點現有設備，預防類似事件再度發生。國研院表示，國研院國網中心在小油坑地區安裝空氣品質感測器，並透過用於防災的無線電波段Band 20，將感測資料傳輸回國網中心主機，驗證Band 20在沒有商用無線網路5G、6G及惡劣環境中，亦可傳輸資料，對於提升防救災工作的通訊傳輸能力有很大幫助；同時藉由空氣品質感測器，收集陽明山區域過去1年8個月的PM 2.5資料，在民生公共物聯網上對外公開，提供各界使用，幫助學研界增加對陽明山地區空氣污染情形的了解。\n",
            "[('起火', 0.3694), ('失火', 0.3694), ('中心', 0.3566), ('火災', 0.3405), ('能力', 0.2752)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 863.03it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 873.09it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "為偵測大屯火山活動噴發氣體與空氣品質，國家實驗研究院國家高速網路與計算中心（國研院國網中心）利用空氣品質感測器，在小油坑地區蒐集大屯山地區的空氣品質資料。\n",
            "小油坑地區位於陽明山國家公園管理範圍內，本計畫於2022年正式向陽明山國家公園管理處提出申請，並獲得同意，程序合法合規，且已於2024年底結束。\n",
            "本計畫係委託廠商架設空氣品質感測器，透過網路線即時回傳空品數據。電力來源是透過太陽能供電系統，保障在偏遠無電區域仍可持續運作。\n",
            "承包廠商以太陽能桿提供空氣品質感測器所需電力，下圖即為太陽能板發電與儲電設備及空氣品質感測器。太陽能桿與空品感測器皆為合法合規之設備，並經測試與驗證後才進行部署。\n",
            "[('中心', 0.3396), ('空氣', 0.333), ('空品', 0.333), ('電力', 0.2869), ('保障', 0.2785)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 295.83it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 265.56it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:05<00:00,  5.43s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "國科會轄下之國家實驗研究院（國研院）與法國原子能暨替代能源總署（CEA）轄下之電子暨資訊技術研究室（CEA-Leti），於臺灣時間4月10日簽訂合作備忘錄，未來將以雙邊工作坊深化技術交流。法國國家健康與醫學研究院（Inserm）則與國研院於4月7日在巴黎辦理「臺法雙邊器官晶片科學論壇」，期能結合法國在基礎醫學實驗與我國在半導體、儀器技術的專長，建立實質的合作研究計畫，並將我國的生物晶片技術推展至歐洲。法國電子暨資訊技術研究室是法國最重要的電子資訊研究單位，專精於微電子和奈米技術應用。國研院與法國電子暨資訊技術研究室未來將在半導體、生醫、光學技術等領域共同舉辦工作坊或研討會，並進行人才交流互訪，期能促進雙方實質技術交流與研究合作。法國國家健康與醫學研究院則是法國首屈一指的生醫研究單位，此次與國研院邀集臺法雙方各12個具藥物開發應用潛力、且已完成概念性驗證或雛型品開發之研究團隊，共同辦理「臺法雙邊器官晶片科學論壇」，就生物材料（Biomaterial）、感測器與影像（Sensor and Imaging）及多功能器官晶片（Multicomponent & Multifunctional OoC）三大主題交流討論，期望透過成果媒合，促成合作研究的機會，加成雙方研發優勢。器官晶片是國科會與法國高等教育暨研究部於2024年臺法科學研究會議中列出的合作項目之一，是全球最受矚目的活體動物替代方法，結合了3D立體細胞培養、微流道、生醫感測器及人工智慧等技術，在實驗室重建器官內部的結構與環境。若模擬人體健康器官功能進行試藥，可用於藥物毒性或副作用的測試；若模擬生病的器官，則有助於藥物功效的測試與篩選；若能建立病人的器官晶片，更有機會直接運用於個人化醫療策略的輔助評估。此次參與論壇的臺灣團隊來自國科會生科處專案補助之「動物實驗替代科技研發計畫」，以及國研院的「器官晶片多元驗證平台」，包括陽明交通大學、清華大學、臺北醫學大學、國家衛生研究院、長庚醫院，以及國研院國家生物模式中心（原國家實驗動物中心）與國家儀器科技研究中心（原台灣儀器科技研究中心）的研究團隊。論壇結束後，國研院蔡宏營院長並率隊參訪法國巴黎－薩克雷大學（UPSaclay）及法國國家科學研究院（CNRS），期能建立臺法雙方更緊密、更全面的合作關係，藉由實習生交流、大型研究設施共享、合作執行研究計畫等，提升臺法雙方在生醫、高速計算、儀器科技領域的科技研發量能。\n",
            "[('原子能', 0.2219), ('cnrs', 0.202), ('微電子', 0.2016), ('方法', 0.2001), ('法國', 0.1958)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 176.09it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.47s/it]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 289.64it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "為使聽障者享有更無礙的聲音感受，國立陽明交通大學、振興醫院與國家實驗研究院國家儀器科技研究中心（國研院國儀中心）合作，成功開發出「微型法布里–珀羅光纖光學麥克風」（Miniaturized Fabry-Perot fiber-optic microphone based on capillary tube and hydrogel diaphragm），解決麥克風受磁場干擾之問題，其研究成果榮登光學領域頂級期刊《Optics & Laser Technology》。此新型微米級光纖麥克風結構簡單、成本低、訊號穩定、尺寸微小，整個麥克風的尺寸如同一根頭髮，而靈敏度比現有技術提升了約37%，能更細微地捕捉聲音變化，同時可以偵測更高頻的聲音，並且兼顧了輕薄微小的功能，未來可應用於穿戴式裝置上。\n",
            "麥克風是一種將聲音轉換成電子訊號的換能器，在生物醫學影像、語音互動系統和助聽器等應用中發揮著至關重要的作用。然而一般麥克風容易受到強電磁場或是射頻干擾，也有電噪聲過大、靈敏度較低的問題。此次發表之新型微米級光纖麥克風，是利用光纖與光學元件來檢測聲波引起的變化，並將這些變化轉換為可測量的光信號。當聲音壓力作用在麥克風的膜片上，造成膜片的變形或振動，改變了光的干涉條紋，藉由此變化轉換為電子信號來偵測聲音。國立陽明交通大學生物醫學工程學系劉承揚教授、振興醫院耳鼻喉部力博宏主任醫師與國研院國儀中心協力合作，首先由力博宏主任醫師發想光纖麥克風的構造概念，接著由國儀中心協助薄膜（麥克風的膜片）製程與檢測，並由劉承揚教授團隊將光纖與薄膜結合，再進行測試與臨床驗證，最終成功開發出新型微米級光纖麥克風。整個麥克風的尺寸如同一根頭髮，同時具有優異的聲音感測靈敏度和穩定性。新型微米級光纖麥克風由於不含金屬材料，因此不受電磁干擾，適用於助聽器和人工電子耳，使用者可以自由進入強電磁場環境而不必擔心噪音的產生，在光聲成像、健康監測、無損檢測、醫學臨床等應用方面亦有具有巨大的商業潛力。\n",
            "國研院國儀中心薄膜製程開發技術之前亦曾協助劉承揚教授團隊成功用蜘蛛絲製成光纖感測器，精準量測糖尿病患者血糖，近期更往高性能半導體材料應用發展。此次協助光纖薄膜製作，再次證明國儀中心是學術界挑戰世界頂尖科技的關鍵夥伴。國儀中心也期待與國內更多學者合作，應用頂尖的薄膜製程開發技術，研發出更多尖端生醫光電醫療器材。\n",
            "[('麥克風', 0.4157), (' fiber-optic', 0.3441), ('光聲', 0.3309), ('光電', 0.3309), ('光纖', 0.3309)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 未用"
      ],
      "metadata": {
        "id": "Sq4F-1DgTaT8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRoErrZVwGMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCtA99OQydVv"
      },
      "source": [
        "# https://www.nltk.org/\n",
        "import nltk\n",
        "import json\n",
        "from google.colab import files # 在 Google Colab 環境上傳檔案時所用；若在個人電腦執行 Python 則不需要\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afVpBzzya7Uu"
      },
      "source": [
        "# 若下載 python 檔並在自己電腦執行，開檔讀檔請用這段 # 若要上傳檔案到 Google Colab 虛擬主機，請註解掉這段\n",
        "#f = open(\"D:/cjlin/sample/ReutersCorn-sample10.json\", \"r\", encoding='UTF-8') # encoding='UTF-8' 才能正確讀入中文檔案\n",
        "#docText = f.read()\n",
        "#f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szej4JgszM_i"
      },
      "source": [
        "# 由電腦上傳檔案至 Colab 虛擬主機 # 若要在個人電腦單機作業，請註解掉這段\n",
        "file = files.upload()\n",
        "trf = open(\"ReutersCorn-train.json\", \"r\", encoding='UTF-8')\n",
        "fdoc = trf.read()\n",
        "trf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfG1FHwyI1aV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYkRVZhDI11E"
      },
      "source": [
        "dataset = json.loads(fdoc)\n",
        "dataset[0]['text'] # 此行僅用來確認執行結果正確，可刪去"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlHjR_iQI66q"
      },
      "source": [
        "file = files.upload()\n",
        "dictf = open(\"posDict_ReutersCorn.json\", \"r\", encoding='UTF-8')\n",
        "dict_raw = dictf.read()\n",
        "dictf.close()\n",
        "POSdict = json.loads(dict_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FccRYuKpJdj0"
      },
      "source": [
        "wordTotal = 0\n",
        "tokenSet = set()\n",
        "for doc in dataset:\n",
        "  word = nltk.word_tokenize(doc['text'])\n",
        "  wordTotal += len(word)\n",
        "  #nltk_token = []\n",
        "  for w in word:\n",
        "    tokenSet.add(w)\n",
        "    #nltk_token.append(w)\n",
        "  #doc['nltk_token'] = nltk_token\n",
        "print(wordTotal)\n",
        "len(tokenSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGBCTag6SYiY"
      },
      "source": [
        "lowerSet = set()\n",
        "for w in tokenSet:\n",
        "  lowerSet.add(w.lower())\n",
        "len(lowerSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-0kTVS0fOxl"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "d7wfjrc_VqZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sets = ['automate', 'automatic', 'automation', 'computer', 'computerize', 'computerization', 'computerizational']\n",
        "for w in word_sets:\n",
        "  stemstr = lemmatizer.lemmatize(w, \"v\")\n",
        "  print(w + ' ==> ' + stemstr)"
      ],
      "metadata": {
        "id": "P5fma8I6Vao6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acqHuXPWVay_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zh1sYiTkVa2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sus2mFI2eIAd"
      },
      "source": [
        "lemmaWOpos = set()\n",
        "for w in lowerSet:\n",
        "  lemmaWOpos.add(lemmatizer.lemmatize(w))\n",
        "len(lemmaWOpos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZdzoz17Lh0B"
      },
      "source": [
        "lemmaWpos = set()\n",
        "for w in lowerSet:\n",
        "  pos = POSdict.get(w)\n",
        "  if pos != None and pos != \"x\":\n",
        "    if pos == \"j\": pos = \"a\"\n",
        "    lemmaWpos.add(lemmatizer.lemmatize(w, pos))\n",
        "  else:\n",
        "    lemmaWpos.add(lemmatizer.lemmatize(w))\n",
        "len(lemmaWpos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWkPiqbif10E"
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BMXL0Kfcxc"
      },
      "source": [
        "stemSet = set()\n",
        "for w in lowerSet:\n",
        "  stemSet.add(stemmer.stem(w))\n",
        "len(stemSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "pxF9xIDYY7Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sets = ['automate', 'automatic', 'automation', 'computer', 'computerize', 'computerization', 'computerizational']\n",
        "for w in word_sets:\n",
        "  stemstr = stemmer.stem(w)\n",
        "  print(w + ' ==> ' + stemstr)\n"
      ],
      "metadata": {
        "id": "F1QesJ85z_te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U ckiptagger[tf,gdown]"
      ],
      "metadata": {
        "id": "FD3Lb0o0apWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER"
      ],
      "metadata": {
        "id": "kYsB82y-awlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_utils.download_data_gdown(\"./\")"
      ],
      "metadata": {
        "id": "ytrXrn2za6oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ws = WS(\"./data\")\n",
        "pos = POS(\"./data\")"
      ],
      "metadata": {
        "id": "J3Qv5BSxbGqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = [\n",
        "    \"土地公有政策?？還是土地婆有政策。.\",\n",
        "    \"某某候選人提出的土地公有政策\",\n",
        "    \"最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.\",\n",
        "    \"電子計算機是會計算題目的機器。\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "gQKcF1JFbToo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_seg = ws([\"高高興興打打球,丟丟看成績如何、開不開心\"])"
      ],
      "metadata": {
        "id": "GFJ6wM8idf_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_seg[0]"
      ],
      "metadata": {
        "id": "myknoTXzdmPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_pos = pos(sent_seg)\n",
        "sent_pos[0]"
      ],
      "metadata": {
        "id": "Iir58IfOmuJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}